<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - theverge_com</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/theverge_com.xml</link>
        <description>A cleaned-up version of the original gaming feed for theverge_com</description>
        <lastBuildDate>Fri, 28 Mar 2025 15:46:54 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Nvidia RTX 5090 mobile GPU:  more efficient and a little faster]]></title>
            <link>https://www.theverge.com/tech/637898/nvidia-rtx-5090-laptop-gpu-impressions-benchmarks-testing-specs</link>
            <guid>https://www.resetera.com/threads/nvidia-rtx-5090-mobile-gpu-more-efficient-and-a-little-faster.1147854/</guid>
            <content:encoded><![CDATA[https://www.theverge.com/tech/637898/nvidia-rtx-5090-laptop-gpu-impressions-benchmarks-testing-specs<br/><br/><div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Nvidia’s 50-series desktop GPUs have been off to a slightly rocky start, with <a href="https://www.theverge.com/news/617901/nvidia-confirms-rare-rtx-5090-and-5070-ti-manufacturing-issue">quality control issues</a>, <a href="https://www.theverge.com/2025/1/23/598045/nvidia-rtx-5080-review-test-benchmark">underwhelming performance</a> gains over last generation, and the occasional <a href="https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues">melting cable</a>. Its new laptop GPUs, led by the flagship RTX 5090, may be on a similarly modest trajectory, but with one added benefit for laptop gamers: efficiency.</p><p>I’ve been testing the RTX 5090 laptop GPU in Razer’s new Blade 16 laptop, which Nvidia is using as a showcase for its top-tier mobile card. My time with it has been more limited than I’d hoped, since my first review unit exhibited some strange graphical anomalies and was prone to blue-screen crashes during basic productivity tasks. Nvidia sent me a replacement, but I’ve had less than two days with it as of press time. From what I’ve seen so far, it is indeed a little faster than the mobile 4090, but like the desktop 50-series cards, its biggest improvements are in DLSS and Multi Frame Generation, rather than raw performance.</p><p>The new mobile <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fgeforce%2Flaptops%2F50-series%2F%23%3A~%3Atext%3DSpecs-%2CGEFORCE%2520RTX%25205090%2520LAPTOP%2520GPU%2C-GEFORCE%2520RTX%25205080" rel="sponsored">RTX 5090</a> has 24GB of GDDR7 VRAM instead of the 16GB of GDDR6 found on the <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fgeforce%2Flaptops%2Fcompare%2F%23%3A~%3Atext%3DGeForce%2520RTX%25204090%2520Laptop%2520GPU" rel="sponsored">4090</a>. It’s also got more powerful Tensor cores, slightly more CUDA cores, and an extra hardware video encoder for better livestreaming and video exports, but the same 175W TDP.</p><div><p><figcaption><em>We’ll have more to say about the laptop housing this new GPU soon.</em></figcaption></p></div><p>The 5090’s specs translate to small gains over the 4090 in synthetic benchmarks, such as a 13 percent higher Geekbench GPU score and a 14 percent higher score in 3DMark’s standard Time Spy test at 2560 x 1440 resolution.</p><p>To test the 5090’s uplift in actual games, I ran an array of benchmarks using <em>Cyberpunk 2077</em> and <em>Black Myth: Wukong</em> — at the native 2560 x 1600 of both Blade 16s and at 4K on an <a href="https://www.theverge.com/2024/1/12/24034953/ces-2024-verge-awards-best-tv-laptop-monitor-gaming-car#:~:text=Best%20monitor-,Alienware%20AW3225QF,-Dell%E2%80%99s%20Alienware%20AW3225QF">Alienware AW3225QF</a> monitor. The biggest difference I saw was in <em>Cyberpunk 2077</em> at 2.5K resolution and Ultra settings without DLSS, frame generation, or ray tracing enabled. There, the 5090 was 24 percent faster than the 4090. That sounds great, but once you enable ray tracing and DLSS 4 (something most people would likely do on either of these laptops), it narrows to a delta of just 5 percent. When you enable 2x frame generation, the 5090 is 14 percent faster than the 4090, and going all out with 4x Multi Frame Gen more than doubles what the 4090 can do at 2x frame gen. If you’re playing at 4K, the 5090 still wins, but the gap narrows to within single digits.</p><div><table><thead><tr><th><h3>Benchmark</h3></th><th><h3>Blade 16 RTX 4090 average fps @ 4K</h3></th><th><h3>Blade 16 RTX 5090 average fps @ 4K</h3></th><th><h3>Blade 16 RTX 4090 average fps @ 2.5K</h3></th><th><h3>Blade 16 RTX 5090 average fps @ 2.5K</h3></th></tr></thead><tbody><tr><td>Black Myth: Wukong TSR</td><td>26</td><td>28</td><td>45</td><td>48</td></tr><tr><td>Black Myth: Wukong FSR</td><td>41</td><td>43</td><td>62</td><td>66</td></tr><tr><td>Black Myth: Wukong FSR Frame Gen</td><td>35</td><td>39</td><td>54</td><td>58</td></tr><tr><td>Black Myth: Wukong DLSS</td><td>46</td><td>48</td><td>68</td><td>70</td></tr><tr><td>Black Myth: Wukong DLSS Frame Gen</td><td>66</td><td>70</td><td>104</td><td>111</td></tr><tr><td>Cyberpunk 2077 Ultra</td><td>41</td><td>48</td><td>74</td><td>92</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; FSR 3.1 Quality</td><td>40</td><td>40</td><td>63</td><td>66</td></tr><tr><td>Cyberpunk 2077 Ultra RT FSR Frame Gen</td><td>61</td><td>70</td><td>118</td><td>124</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS quality</td><td>39</td><td>40</td><td>64</td><td>67</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x2</td><td>67</td><td>70</td><td>103</td><td>117</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x4</td><td>N/A</td><td>123</td><td>N/A</td><td>209</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT no DLSS</td><td>12</td><td>12</td><td>21</td><td>23</td></tr><tr><td>Cyberpunk 2077 Full Ultra &amp; DLSS Quality</td><td>23</td><td>25</td><td>39</td><td>45</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT &amp; DLSS 4 Quality Frame Gen x2</td><td>43</td><td>45</td><td>67</td><td>81</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT &amp; DLSS 4 Quality Frame Gen x4</td><td>N/A</td><td>83</td><td>N/A</td><td>147</td></tr></tbody></table></div><p><em>Black Myth: Wukong</em>, on the other hand, is a little less forgiving. In this graphically demanding Souls-adjacent RPG, the 5090 is only a few percent faster than the 4090. The highest average the 5090 could muster was 111fps at 2.5K with DLSS and standard frame generation, with the 4090 wasn’t far behind at 104. If <em>Wukong</em> supported Multi Frame Generation, the 5090 could no doubt separate itself further, but that would likely <a href="https://youtu.be/xpzufsxtZpA?si=09GBCKtpAafpCCRZ&amp;t=668">add a couple extra milliseconds of latency</a>. Most people will never notice or care about that, but some players may feel any latency added to their pinpoint dodge-roll timing is a nonstarter.</p><p>Like the 50-series desktop cards, the 5090 laptop GPU looks its best in games that support DLSS 4 Multi Frame Generation. Over 100 games now support Nvidia’s highlight feature, and you can also use override settings in the Nvidia app to force it in unsupported games. </p><p>Frame generation, which both Nvidia and AMD offer, uses AI models to generate extra frames and insert them between traditionally rendered frames. This gives the appearance of smoother gameplay but doesn’t improve input latency — and actually increases it slightly. AMD and previous Nvidia cards can do 2x frame generation; the 50 series can do up to 4x. There’s an ongoing debate in the world of PC gaming about these “fake frames” — some folks are perfectly content with AI upscaling and generated frames, and others view it as a band-aid solution with too many compromises to image quality and game feel. I think it depends on your personal experience, what kinds of games you like to play and what you’re used to playing them on, and even whether you have an eye (or even a care) for this stuff. It can be great for the right games, and it may even make more sense on a 16-inch laptop screen than a jumbo 4K monitor right in front of your face. </p><p>I forced Multi Frame Generation on <a href="https://www.theverge.com/games-review/631744/assassins-creed-shadows-review-ubisoft"><em>Assassin’s Creed Shadows</em></a>, and I was pleasantly surprised with its performance and buttery smoothness on ultra settings. <em>Shadows </em>is maybe a perfect example of a single-player game where frame generation is a nonissue. It’s got dodge rolling, but it’s not the kind of game that really requires frame-perfect timing. It felt A-okay to me, and boy did feudal Japan look lovely on a 2.5K OLED at an average of 167fps.</p><div><p><figcaption><em>Many of those frames may be “fake,” but they sure are pretty.</em></figcaption></p></div><p>The obvious games to avoid with frame generation are competitive shooters. While I felt fine playing <em>Marvel Rivals</em> on the 5090 with 4x frame generation reaching nearly 200fps, I was playing pretty casually with a hero that doesn’t require precision accuracy. For something like <em>Valorant </em>or <em>Counter-Strike 2</em>, I’d advise against it. But to be fair, those ultra-competitive games typically have a low performance barrier for entry and practically run on a potato.</p><p>One area where the 5090 does seem significantly better so far is power efficiency. In our benchmarks, the new GPU had a lower average wattage, by 20 to 29 percent, than the 4090. That should help improve battery life when playing games away from a power cord.</p><div><h2>GPU power (watts) at 2.5K</h2><table><thead><tr><th><h3>Benchmarks</h3></th><th><h3>Blade 16 RTX 4090 average GPU wattage</h3></th><th><h3>Blade 16 RTX 5090 average GPU wattage</h3></th></tr></thead><tbody><tr><td>Black Myth: Wukong TSR</td><td>174</td><td>138</td></tr><tr><td>Black Myth: Wukong FSR</td><td>171</td><td>137</td></tr><tr><td>Black Myth: Wukong FSR Frame Gen</td><td>145</td><td>134</td></tr><tr><td>Black Myth: Wukong DLSS</td><td>170</td><td>135</td></tr><tr><td>Black Myth: Wukong DLSS Frame Gen</td><td>163</td><td>131</td></tr><tr><td>Cyberpunk 2077 Ultra</td><td>159</td><td>134</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; FSR 3.1 quality</td><td>153</td><td>127</td></tr><tr><td>Cyberpunk 2077 Ultra RT FSR Frame Gen</td><td>154</td><td>133</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Quality</td><td>154</td><td>126</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x2</td><td>162</td><td>130</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x4</td><td>N/A</td><td>133</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT no DLSS</td><td>169</td><td>131</td></tr><tr><td>Cyberpunk 2077 Full Ultra &amp; DLSS quality</td><td>167</td><td>138</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT &amp; DLSS 4 Quality Frame Gen x2</td><td>167</td><td>137</td></tr><tr><td>Cyberpunk 2077 Full UltraA RT &amp; DLSS 4 Quality Frame Gen x4</td><td>N/A</td><td>133</td></tr></tbody></table></div><p>Lower wattages can also mean lower temperatures, as I was able to play games with the plugged-in laptop on my actual lap and not toast my legs. But this also hinges on an individual laptop’s thermal design and how well it cools — the Blade 16 so far seems to do it well, but it’s also a totally different design than the 2024 model with the 4090 in it. Of course, you’re not going to see the same level of performance when running graphically intensive games on battery power. I’ll have to test this more to see how much of a difference it makes in real-world usage.</p><p>The RTX 5090 is Nvidia’s most powerful and expensive laptop GPU, and you can expect to pay $4,000 or more for a laptop that runs it. The Blade 16 review configuration costs <a href="https://razer.a9yw.net/c/482924/642901/10229?u=https%3A%2F%2Fwww.razer.com%2Fgaming-laptops%2FRazer-Blade-16%2FRZ09-05289EN4-R3U1" rel="sponsored">$4,499.99</a>. It shows some promise for games that support DLSS 4 and Multi Frame Generation, and maybe for gaming that’s not always tethered to a wall. But if I were shopping right now, I wouldn’t ignore a heavy discount on a laptop with an RTX 4090. (The 4090 desktop supply, on the other hand, has dried up.)</p><p>We’ll continue to test the RTX 5090, as well as the rest of the 50-series mobile GPUs as they appear in new gaming laptop models over the next few weeks and months. I’m looking forward to testing the RTX 5080, in particular, and seeing if that’s the sweet spot for laptop gamers.</p><p><em>Photography by Antonio G. Di Benedetto / The Verge</em></p></div></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nintendo’s Legend of Zelda movie hits theaters in March 2027]]></title>
            <link>https://www.theverge.com/news/638133/legend-of-zelda-movie-premiere-date-2027</link>
            <guid>https://www.resetera.com/threads/nintendo%E2%80%99s-legend-of-zelda-movie-hits-theaters-in-march-2027.1147782/</guid>
            <content:encoded><![CDATA[https://www.theverge.com/news/638133/legend-of-zelda-movie-premiere-date-2027<br/><br/><div id="readability-page-1" class="page"><div id="__next"><p><a href="#content"><span>Skip to main content</span></a></p><nav aria-label="Sticky Nav"></nav><div><main id="content"><article><div><div><div><p><span><ul><li><a href="https://www.theverge.com/news">News</a></li></ul></span></p></div><div><p>﻿There’s still not much info available about the live-action film.</p></div></div><div><p>﻿There’s still not much info available about the live-action film.</p><div><p><span><time datetime="2025-03-28T13:16:29+00:00">Mar 28, 2025, 1:16 PM UTC</time></span></p></div></div></div><div><div><div><p><a href="https://www.theverge.com/authors/andrew-webster">Andrew Webster</a> <span>is an entertainment editor covering streaming, virtual worlds, and every single Pokémon video game. Andrew joined The Verge in 2012, writing over 4,000 stories.</span></p></div><div id="zephr-anchor"><p>Nintendo’s <a href="https://www.theverge.com/news/637319/nintendo-today-news-app">news app</a> is already breaking some news: today it revealed the premiere date for the upcoming live-action <em>Zelda</em> movie. The film, which appears to be called simply <em>The Legend of Zelda</em>, will hit theaters worldwide on March 26th, 2027.</p><p>Here’s a screenshot of the reveal:</p><div><p>We still know next to nothing about the movie, which <a href="https://www.theverge.com/2023/11/7/23951339/the-legend-of-zelda-movie-live-action-nintendo">was first announced in 2023</a> following the breakout success of the animated <em>Super Mario Bros. Movie</em>, which <a href="https://www.theverge.com/2023/5/1/23695720/nintendo-mario-movie-billion-box-office">earned more than $1 billion at the box office</a>. It’ll be directed by Wes Ball (<em><a href="https://www.theverge.com/2024/5/10/24152293/kingdom-of-the-planet-of-the-apes-review">Kingdom of the Planet of the Apes</a></em>), while Zelda and Mario creator Shigeru Miyamoto will serve as producer, much as he did on the <em>Super Mario</em> movie.</p></div></div></div><div><form><div><h2>Installer</h2><p><span>A weekly newsletter by David Pierce designed to tell you everything you need to download, watch, read, listen to, and explore that fits in The Verge’s universe.</span></p></div></form></div></div></article></main></div></div></div>]]></content:encoded>
        </item>
    </channel>
</rss>