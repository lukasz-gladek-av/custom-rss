<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - theverge_com</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/theverge_com.xml</link>
        <description>A cleaned-up version of the original gaming feed for theverge_com</description>
        <lastBuildDate>Fri, 28 Mar 2025 16:25:42 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Helldivers 2 is locking out players who use older CPUs]]></title>
            <link>https://www.theverge.com/news/638317/helldivers-2-avx2-cpu-requirement-lockout-sony</link>
            <guid>https://www.resetera.com/threads/helldivers-2-is-locking-out-players-who-use-older-cpus.1147968/</guid>
            <content:encoded><![CDATA[https://www.theverge.com/news/638317/helldivers-2-avx2-cpu-requirement-lockout-sony<br/><br/><div id="readability-page-1" class="page"><div><div><p><a href="https://www.theverge.com/authors/umar-shakir">Umar Shakir</a> <span>is a news writer fond of the electric vehicle lifestyle and things that plug in via USB-C. He spent over 15 years in IT support before joining The Verge.</span></p></div><div id="zephr-anchor"><p>Some <em>Helldivers 2</em> players are reporting that they can no longer play the game on their computers equipped with older CPUs. The issue stems from the developer Arrowhead Game Studios’ and publisher Sony’s requirement that users play on systems that support AVX2 (Advanced Vector Extensions), which has been a feature in many CPUs since 2013. However, the requirement was not properly enforced in the software, allowing some players with older hardware to play the game since it was launched for PC last year.</p><p>As reported by <a href="https://www.eurogamer.net/helldivers-2-suddenly-unplayable-for-some-pc-players-on-older-systems"><em>Eurogamer</em></a>, players on the <em>Helldivers 2</em> Discord server have voiced their displeasure with suddenly not being able to launch a game they have paid for and have been playing. “600+ hours in a game I love just to be locked out is making me so frustrated to the point I just want to ask for a refund instead of fixing their mistakes,” said a user.</p><p>Server moderator Birby responded to some of those complaining and explained that “the AVX2 check in the script is experimental, and can throw false positives.” The moderator blamed users for not following the stated minimum system requirements. “I don’t like to have to tell people their 13 year old processor is out of date,” said the moderator. “You should never be surprised if your out of spec machine can’t run the game.”</p><p>Still, players with the older CPUs likely have PCs or laptops that aren’t upgradable, so they will need to prepare to spend a lot of money on a totally new system.</p><p>This isn’t Sony’s first unpopular move made on PC <em>Helldivers 2</em> players. Last year the company had forced PSN account requirements for PC users, but then <a href="https://www.theverge.com/24150327/sony-helldivers-2-review-bombing-psn-login">pulled back after all the negative reviews on Steam</a>.</p></div></div></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[In Baby Steps, I stumbled through a refreshing kind of open world]]></title>
            <link>https://www.theverge.com/games/638179/baby-steps-preview-ps5-steam</link>
            <guid>https://www.resetera.com/threads/in-baby-steps-i-stumbled-through-a-refreshing-kind-of-open-world.1147965/</guid>
            <content:encoded><![CDATA[https://www.theverge.com/games/638179/baby-steps-preview-ps5-steam<br/><br/><div id="readability-page-1" class="page"><div id="zephr-anchor"><p>“Is the rage boiling over?” Bennett Foddy asks me as I fall down a wooden plank for the dozenth time. The creator of <em>QWOP</em> and <em>Getting Over It</em> is known for games that can be both frustrating and meditative, pairing challenging physics with thoughtful design. The latest, <em>Baby Steps</em>, is similar — but a whole lot bigger.</p><p>I had a chance to play a chunk of the early portion of <em>Baby Steps</em> at 2025’s Game Developers Conference earlier this month. The game is developed by a trio made up of Foddy, Maxi Boch, and <a href="https://www.theverge.com/2019/3/8/18255975/ape-out-game-pc-nintendo-switch"><em>Ape Out</em></a> creator Gabe Cuzzillo. It starts out pretty silly: you play as a slacker named Nate who is transported to a strange world with little choice but to hike. There are cutscenes where he babbles in confusion at his situation and a handful of characters who aren’t especially helpful. At one point, someone offers Nate a map, and he refuses; there is no gear that aids your adventure in <em>Baby Steps</em>.</p><p>Instead, the game is perhaps the most literal definition of a walking simulator. You move by controlling Nate’s two legs individually to take steps. At first, I couldn’t take more than a few steps before falling flat on my face, which, to be fair, was often hilarious. The physics makes the wipeouts in <em>Baby Steps</em> entertaining to watch. But it was also infuriating. Walking is supposed to be the easy part of a video game; you push the stick forward and go where you want to. But here, even the tiniest obstacle proved troublesome. The first time I saw a small staircase, I wondered how I’d ever make it.</p><p>But slowly, through a lot of trial and error, I fell into a rhythm. It reminded me a bit of <a href="https://www.theverge.com/2019/11/1/20941606/death-stranding-review-ps4-hideo-kojima"><em>Death Stranding</em></a>, where I had to carefully pay attention to my movements if I wanted to get anywhere. For just regular walking up a path, this meant timing my steps just right so that I didn’t stumble. For obstacles like logs or staircases, it meant taking my time and carefully placing my foot where it needed to go. It never got easy, but at least I felt in control of my movements after a while.</p><p><em>Baby Steps</em> takes place in what seems like a fairly large and open world, the kind of place where you can see something on the horizon and go there. The trick, of course, is being able to actually walk toward it. Early on, I came to a fork in the road: on one side was a gently sloping hill that led further up the mountain; on the other, a wooden plank across a muddy slope that led right back down to the bottom. After some deliberation, I attempted to get across the plank and, two steps later, found myself sliding in the mud.</p><p>The developers tell me that <em>Baby Steps</em> is built on the idea of “self guided play,” meaning there are no artificial obstacles in your way, and you don’t have to worry about things like health or death. It’s just you and the environment. And while it might not seem like it at first, the game isn’t really designed to be punishing. As an example, when I had my big fall down the mountain, it didn’t send me back to the beginning but instead brought me somewhere new that I wouldn’t have seen otherwise. It also taught me an important lesson about taking on challenges I’m ready for.</p><p>I played for less than an hour, but I could already feel myself settling into a contemplative style of exploration I’ve experienced in games like <em>Death Stranding</em> and <a href="https://www.theverge.com/2017/3/2/14787082/the-legend-of-zelda-breath-of-the-wild-review"><em>Breath of the Wild</em></a>. There were no quests to complete or to-do lists to check off. Just me, my legs, and a serene mountain full of risks and rewards.</p><p><em>Baby Steps</em> launches later this year on Steam and the PS5.</p></div></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Nvidia RTX 5090 mobile GPU:  more efficient and a little faster]]></title>
            <link>https://www.theverge.com/tech/637898/nvidia-rtx-5090-laptop-gpu-impressions-benchmarks-testing-specs</link>
            <guid>https://www.resetera.com/threads/nvidia-rtx-5090-mobile-gpu-more-efficient-and-a-little-faster.1147854/</guid>
            <content:encoded><![CDATA[https://www.theverge.com/tech/637898/nvidia-rtx-5090-laptop-gpu-impressions-benchmarks-testing-specs<br/><br/><div id="readability-page-1" class="page"><div id="zephr-anchor"><p>Nvidia’s 50-series desktop GPUs have been off to a slightly rocky start, with <a href="https://www.theverge.com/news/617901/nvidia-confirms-rare-rtx-5090-and-5070-ti-manufacturing-issue">quality control issues</a>, <a href="https://www.theverge.com/2025/1/23/598045/nvidia-rtx-5080-review-test-benchmark">underwhelming performance</a> gains over last generation, and the occasional <a href="https://www.theverge.com/news/609207/nvidia-rtx-5090-power-connector-melting-burning-issues">melting cable</a>. Its new laptop GPUs, led by the flagship RTX 5090, may be on a similarly modest trajectory, but with one added benefit for laptop gamers: efficiency.</p><p>I’ve been testing the RTX 5090 laptop GPU in Razer’s new Blade 16 laptop, which Nvidia is using as a showcase for its top-tier mobile card. My time with it has been more limited than I’d hoped, since my first review unit exhibited some strange graphical anomalies and was prone to blue-screen crashes during basic productivity tasks. Nvidia sent me a replacement, but I’ve had less than two days with it as of press time. From what I’ve seen so far, it is indeed a little faster than the mobile 4090, but like the desktop 50-series cards, its biggest improvements are in DLSS and Multi Frame Generation, rather than raw performance.</p><p>The new mobile <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fgeforce%2Flaptops%2F50-series%2F%23%3A~%3Atext%3DSpecs-%2CGEFORCE%2520RTX%25205090%2520LAPTOP%2520GPU%2C-GEFORCE%2520RTX%25205080" rel="sponsored">RTX 5090</a> has 24GB of GDDR7 VRAM instead of the 16GB of GDDR6 found on the <a href="https://go.skimresources.com/?id=1025X1701640&amp;xs=1&amp;url=https%3A%2F%2Fwww.nvidia.com%2Fen-us%2Fgeforce%2Flaptops%2Fcompare%2F%23%3A~%3Atext%3DGeForce%2520RTX%25204090%2520Laptop%2520GPU" rel="sponsored">4090</a>. It’s also got more powerful Tensor cores, slightly more CUDA cores, and an extra hardware video encoder for better livestreaming and video exports, but the same 175W TDP.</p><div><p><figcaption><em>We’ll have more to say about the laptop housing this new GPU soon.</em></figcaption></p></div><p>The 5090’s specs translate to small gains over the 4090 in synthetic benchmarks, such as a 13 percent higher Geekbench GPU score and a 14 percent higher score in 3DMark’s standard Time Spy test at 2560 x 1440 resolution.</p><p>To test the 5090’s uplift in actual games, I ran an array of benchmarks using <em>Cyberpunk 2077</em> and <em>Black Myth: Wukong</em> — at the native 2560 x 1600 of both Blade 16s and at 4K on an <a href="https://www.theverge.com/2024/1/12/24034953/ces-2024-verge-awards-best-tv-laptop-monitor-gaming-car#:~:text=Best%20monitor-,Alienware%20AW3225QF,-Dell%E2%80%99s%20Alienware%20AW3225QF">Alienware AW3225QF</a> monitor. The biggest difference I saw was in <em>Cyberpunk 2077</em> at 2.5K resolution and Ultra settings without DLSS, frame generation, or ray tracing enabled. There, the 5090 was 24 percent faster than the 4090. That sounds great, but once you enable ray tracing and DLSS 4 (something most people would likely do on either of these laptops), it narrows to a delta of just 5 percent. When you enable 2x frame generation, the 5090 is 14 percent faster than the 4090, and going all out with 4x Multi Frame Gen more than doubles what the 4090 can do at 2x frame gen. If you’re playing at 4K, the 5090 still wins, but the gap narrows to within single digits.</p><div><table><thead><tr><th><h3>Benchmark</h3></th><th><h3>Blade 16 RTX 4090 average fps @ 4K</h3></th><th><h3>Blade 16 RTX 5090 average fps @ 4K</h3></th><th><h3>Blade 16 RTX 4090 average fps @ 2.5K</h3></th><th><h3>Blade 16 RTX 5090 average fps @ 2.5K</h3></th></tr></thead><tbody><tr><td>Black Myth: Wukong TSR</td><td>26</td><td>28</td><td>45</td><td>48</td></tr><tr><td>Black Myth: Wukong FSR</td><td>41</td><td>43</td><td>62</td><td>66</td></tr><tr><td>Black Myth: Wukong FSR Frame Gen</td><td>35</td><td>39</td><td>54</td><td>58</td></tr><tr><td>Black Myth: Wukong DLSS</td><td>46</td><td>48</td><td>68</td><td>70</td></tr><tr><td>Black Myth: Wukong DLSS Frame Gen</td><td>66</td><td>70</td><td>104</td><td>111</td></tr><tr><td>Cyberpunk 2077 Ultra</td><td>41</td><td>48</td><td>74</td><td>92</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; FSR 3.1 Quality</td><td>40</td><td>40</td><td>63</td><td>66</td></tr><tr><td>Cyberpunk 2077 Ultra RT FSR Frame Gen</td><td>61</td><td>70</td><td>118</td><td>124</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS quality</td><td>39</td><td>40</td><td>64</td><td>67</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x2</td><td>67</td><td>70</td><td>103</td><td>117</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x4</td><td>N/A</td><td>123</td><td>N/A</td><td>209</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT no DLSS</td><td>12</td><td>12</td><td>21</td><td>23</td></tr><tr><td>Cyberpunk 2077 Full Ultra &amp; DLSS Quality</td><td>23</td><td>25</td><td>39</td><td>45</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT &amp; DLSS 4 Quality Frame Gen x2</td><td>43</td><td>45</td><td>67</td><td>81</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT &amp; DLSS 4 Quality Frame Gen x4</td><td>N/A</td><td>83</td><td>N/A</td><td>147</td></tr></tbody></table></div><p><em>Black Myth: Wukong</em>, on the other hand, is a little less forgiving. In this graphically demanding Souls-adjacent RPG, the 5090 is only a few percent faster than the 4090. The highest average the 5090 could muster was 111fps at 2.5K with DLSS and standard frame generation, with the 4090 wasn’t far behind at 104. If <em>Wukong</em> supported Multi Frame Generation, the 5090 could no doubt separate itself further, but that would likely <a href="https://youtu.be/xpzufsxtZpA?si=09GBCKtpAafpCCRZ&amp;t=668">add a couple extra milliseconds of latency</a>. Most people will never notice or care about that, but some players may feel any latency added to their pinpoint dodge-roll timing is a nonstarter.</p><p>Like the 50-series desktop cards, the 5090 laptop GPU looks its best in games that support DLSS 4 Multi Frame Generation. Over 100 games now support Nvidia’s highlight feature, and you can also use override settings in the Nvidia app to force it in unsupported games. </p><p>Frame generation, which both Nvidia and AMD offer, uses AI models to generate extra frames and insert them between traditionally rendered frames. This gives the appearance of smoother gameplay but doesn’t improve input latency — and actually increases it slightly. AMD and previous Nvidia cards can do 2x frame generation; the 50 series can do up to 4x. There’s an ongoing debate in the world of PC gaming about these “fake frames” — some folks are perfectly content with AI upscaling and generated frames, and others view it as a band-aid solution with too many compromises to image quality and game feel. I think it depends on your personal experience, what kinds of games you like to play and what you’re used to playing them on, and even whether you have an eye (or even a care) for this stuff. It can be great for the right games, and it may even make more sense on a 16-inch laptop screen than a jumbo 4K monitor right in front of your face. </p><p>I forced Multi Frame Generation on <a href="https://www.theverge.com/games-review/631744/assassins-creed-shadows-review-ubisoft"><em>Assassin’s Creed Shadows</em></a>, and I was pleasantly surprised with its performance and buttery smoothness on ultra settings. <em>Shadows </em>is maybe a perfect example of a single-player game where frame generation is a nonissue. It’s got dodge rolling, but it’s not the kind of game that really requires frame-perfect timing. It felt A-okay to me, and boy did feudal Japan look lovely on a 2.5K OLED at an average of 167fps.</p><div><p><figcaption><em>Many of those frames may be “fake,” but they sure are pretty.</em></figcaption></p></div><p>The obvious games to avoid with frame generation are competitive shooters. While I felt fine playing <em>Marvel Rivals</em> on the 5090 with 4x frame generation reaching nearly 200fps, I was playing pretty casually with a hero that doesn’t require precision accuracy. For something like <em>Valorant </em>or <em>Counter-Strike 2</em>, I’d advise against it. But to be fair, those ultra-competitive games typically have a low performance barrier for entry and practically run on a potato.</p><p>One area where the 5090 does seem significantly better so far is power efficiency. In our benchmarks, the new GPU had a lower average wattage, by 20 to 29 percent, than the 4090. That should help improve battery life when playing games away from a power cord.</p><div><h2>GPU power (watts) at 2.5K</h2><table><thead><tr><th><h3>Benchmarks</h3></th><th><h3>Blade 16 RTX 4090 average GPU wattage</h3></th><th><h3>Blade 16 RTX 5090 average GPU wattage</h3></th></tr></thead><tbody><tr><td>Black Myth: Wukong TSR</td><td>174</td><td>138</td></tr><tr><td>Black Myth: Wukong FSR</td><td>171</td><td>137</td></tr><tr><td>Black Myth: Wukong FSR Frame Gen</td><td>145</td><td>134</td></tr><tr><td>Black Myth: Wukong DLSS</td><td>170</td><td>135</td></tr><tr><td>Black Myth: Wukong DLSS Frame Gen</td><td>163</td><td>131</td></tr><tr><td>Cyberpunk 2077 Ultra</td><td>159</td><td>134</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; FSR 3.1 quality</td><td>153</td><td>127</td></tr><tr><td>Cyberpunk 2077 Ultra RT FSR Frame Gen</td><td>154</td><td>133</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Quality</td><td>154</td><td>126</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x2</td><td>162</td><td>130</td></tr><tr><td>Cyberpunk 2077 Ultra RT &amp; DLSS Frame Gen x4</td><td>N/A</td><td>133</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT no DLSS</td><td>169</td><td>131</td></tr><tr><td>Cyberpunk 2077 Full Ultra &amp; DLSS quality</td><td>167</td><td>138</td></tr><tr><td>Cyberpunk 2077 Full Ultra RT &amp; DLSS 4 Quality Frame Gen x2</td><td>167</td><td>137</td></tr><tr><td>Cyberpunk 2077 Full UltraA RT &amp; DLSS 4 Quality Frame Gen x4</td><td>N/A</td><td>133</td></tr></tbody></table></div><p>Lower wattages can also mean lower temperatures, as I was able to play games with the plugged-in laptop on my actual lap and not toast my legs. But this also hinges on an individual laptop’s thermal design and how well it cools — the Blade 16 so far seems to do it well, but it’s also a totally different design than the 2024 model with the 4090 in it. Of course, you’re not going to see the same level of performance when running graphically intensive games on battery power. I’ll have to test this more to see how much of a difference it makes in real-world usage.</p><p>The RTX 5090 is Nvidia’s most powerful and expensive laptop GPU, and you can expect to pay $4,000 or more for a laptop that runs it. The Blade 16 review configuration costs <a href="https://razer.a9yw.net/c/482924/642901/10229?u=https%3A%2F%2Fwww.razer.com%2Fgaming-laptops%2FRazer-Blade-16%2FRZ09-05289EN4-R3U1" rel="sponsored">$4,499.99</a>. It shows some promise for games that support DLSS 4 and Multi Frame Generation, and maybe for gaming that’s not always tethered to a wall. But if I were shopping right now, I wouldn’t ignore a heavy discount on a laptop with an RTX 4090. (The 4090 desktop supply, on the other hand, has dried up.)</p><p>We’ll continue to test the RTX 5090, as well as the rest of the 50-series mobile GPUs as they appear in new gaming laptop models over the next few weeks and months. I’m looking forward to testing the RTX 5080, in particular, and seeing if that’s the sweet spot for laptop gamers.</p><p><em>Photography by Antonio G. Di Benedetto / The Verge</em></p></div></div>]]></content:encoded>
        </item>
    </channel>
</rss>