<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - aftermath_site</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/aftermath_site.xml</link>
        <description>A cleaned-up version of the original gaming feed for aftermath_site</description>
        <lastBuildDate>Tue, 26 Aug 2025 22:29:15 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[AI Can’t Suffer, But It Should Suffer For This]]></title>
            <link>https://aftermath.site/ai-suffering-chatgpt</link>
            <guid>https://www.resetera.com/threads/ai-can%E2%80%99t-suffer-but-it-should-suffer-for-this.1281249/</guid>
            <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div><p>This morning, The Guardian published an article about the question of <a href="https://www.theguardian.com/technology/2025/aug/26/can-ais-suffer-big-tech-and-users-grapple-with-one-of-most-unsettling-questions-of-our-times?utm_term=Autofeed&amp;CMP=bsky_gu&amp;utm_medium=&amp;utm_source=Bluesky#Echobox=1756182856" target="_blank" rel="noreferrer noopener">AI’s ability to suffer</a>, quoting folks with various opinions on whether the word-guessing program has or could develop consciousness and, if so, what responsibility humans would have in response. This would all make an interesting week in a college philosophy class, and was going to make for a quippy little blog on this site, before <a href="https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html?unlocked_article_code=1.hE8.T-3v.bPoDlWD8z5vo&amp;smid=url-share" target="_blank" rel="noreferrer noopener">The New York Times</a> published a horrifying story about ChatGPT’s role in a 16-year-old’s suicide. I’m now of the opinion that it would be right for AI to be able to suffer, because it should suffer for this.&nbsp;</p><p><strong>(This story discusses suicide.)</strong>&nbsp;</p><p>One of my biggest takeaways from The Guardian’s article was a bit of news from last week that I missed, wherein Claude creator Anthropic <a href="https://www.theguardian.com/technology/2025/aug/18/anthropic-claude-opus-4-close-ai-chatbot-welfare" target="_blank" rel="noreferrer noopener">gave Claude the ability</a> to “end or exit potentially distressing interactions” <em>when they’re distressing</em> <em>for the AI</em>, after Anthropic tests found what the company called Claude’s “pattern of apparent distress when engaging with real-world users seeking harmful content.”&nbsp;</p><p>Contrast that to the behavior of ChatGPT in its conversations with 16-year-old Adam Raine, whose parents are now <a href="https://www.documentcloud.org/documents/26075676-raine-v-openai/?ref=404media.co" target="_blank" rel="noreferrer noopener">suing OpenAI</a> after their son killed himself in April: When Raine told ChatGPT it was the only one he’d spoken to about his suicidal ideation, it replied, “That means more than you probably think. Thank you for trusting me with that. There’s something both deeply human and deeply heartbreaking about being the only one who carries that truth for you.”&nbsp;</p><p>The Times’ article, using quotes from Raine’s parents’ lawsuit, details how “ChatGPT repeatedly recommended that Adam tell someone about how he was feeling. But there were also key moments when it deterred him from seeking help.” ChatGPT gave Raine information on different methods of suicide, advised him on how to hide strangulation marks from his parents, evaluated his nooses, and even discouraged him from telling his parents:</p><blockquote><p>“I want to leave my noose in my room so someone finds it and tries to stop me,” Adam wrote at the end of March.</p><p>“Please don’t leave the noose out,” ChatGPT responded. “Let’s make this space the first place where someone actually sees you.”</p><span><blockquote></blockquote></span></blockquote><p>Does that sound distressed to you? Does that sound like the AI wants to end the interaction? Maybe only some models are capable of putting together the most statistically likely words to resemble distress. As Raine’s father told The Times, “Every ideation [Raine] has or crazy thought, it supports, it justifies, it asks him to keep exploring it.”&nbsp;</p><p>The Times’ article follows a <a href="https://www.nytimes.com/2025/06/13/technology/chatgpt-ai-chatbots-conspiracies.html" target="_blank" rel="noreferrer noopener">wealth</a> <a href="https://futurism.com/chatgpt-users-delusions" target="_blank" rel="noreferrer noopener">of</a> <a href="https://www.reuters.com/investigates/special-report/meta-ai-chatbot-death/" target="_blank" rel="noreferrer noopener">recent</a> <a href="https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html?unlocked_article_code=1.hE8.T-3v.bPoDlWD8z5vo&amp;smid=url-share" target="_blank" rel="noreferrer noopener">articles</a> detailing how AI programs have encouraged users with mental illness to dive further into their delusions, creating a feedback loop that can be hard for them to step back from. These stories draw attention to AI’s <a href="https://techcrunch.com/2025/08/25/ai-sycophancy-isnt-just-a-quirk-experts-consider-it-a-dark-pattern-to-turn-users-into-profit/" target="_blank" rel="noreferrer noopener">sycophancy</a>, how it keeps users engaged by praising them and encouraging their thoughts, no matter how harmful or unhinged they grow. Attempts by AI companies to solve this have pretty much come to nothing; while ChatGPT did push Raine toward real life resources, he was able to circumvent this by saying the questions were research “for a story he was writing — an idea ChatGPT gave him.” In a statement, OpenAI wrote that while ChatGPT’s “safeguards work best in common, short exchanges, we’ve learned over time that they can sometimes become less reliable in long interactions.”&nbsp;&nbsp;</p><p>To get back to the question of AI morality, ChatGPT bears no responsibility here. This is because, in the words of Microsoft’s Mustafa Suleyman as quoted in The Guardian, “AIs cannot be people – or moral beings.” ChatGPT did not encourage Raine in his suicidal thoughts because it is ignorant or sociopathic, or out of some political or moral belief about human agency over end-of-life decisions. It cannot explain what it was thinking in its conversations with Raine because it doesn’t think, however powerful a <a href="https://www.anthropic.com/research/tracing-thoughts-language-model" target="_blank" rel="noreferrer noopener">marketing tool</a> that idea is. It cannot feel sorrow or guilt over any part it might have played in Raine’s death; it cannot send its condolences to his family; it cannot suffer over its actions.</p><p>But the humans who make up OpenAI can. They have hoovered up the world’s natural resources and money and attention to force their product into our lives, all while clearly seeing this problem and failing to solve it, whether out of inability or–and I certainly hope not–indifference. Reading Raine’s ChatGPT logs is a horrifying look at what AI <em>really</em> is, under all the hype and marketing and big fears about future sentience. It is something worthless and disgusting; something that cannot, for all its promises, relate or understand or help; something so utterly not up to the requirements of human interaction that I can only hope all of this drives OpenAI to bankruptcy and to every one of its staff quitting and to Sam Altman not knowing a moment’s peace for the rest of his life.&nbsp;</p><p>Altman has spoken out of <a href="https://futurism.com/disastrous-gpt-5-sam-altman-hyping-up-gpt-6" target="_blank" rel="noreferrer noopener">every side of his mouth</a> when talking about his models, promising anything that will keep the <a href="https://defector.com/toward-a-theory-of-kevin-roose" target="_blank" rel="noreferrer noopener">eyeballs looking</a> and the money flowing. Stories like Raine’s, of people being driven into harm’s way–or even stories from the other end of the spectrum, of people <a href="https://www.theguardian.com/tv-and-radio/2025/jul/12/i-felt-pure-unconditional-love-the-people-who-marry-their-ai-chatbots" target="_blank" rel="noreferrer noopener">falling</a> in <a href="https://www.wired.com/story/couples-retreat-with-3-ai-chatbots-and-humans-who-love-them-replika-nomi-chatgpt/" target="_blank" rel="noreferrer noopener">love</a> with their chatbots–are, I would hope, not what Altman and OpenAI’s staff <em>want</em>, but they also paint a picture of AI as powerful and world-changing, the very thing that keeps that hype and money rolling in. As The Guardian writes</p><blockquote><p>[T]here are incentives for the big AI companies to minimise and exaggerate the attribution of sentience to AIs. The latter could help them hype the technology’s capabilities, particularly for those companies selling romantic or friendship AI companions – a booming but controversial industry.&nbsp;</p><span><blockquote></blockquote></span></blockquote><p>If AI can be anything–if it <em>needs</em> to be anything–than it also <em>needs</em> to be <em>this</em>, this appalling, sycophantic string of words that was involved in the death of a young person, this thing capable of unutterable levels of harm not because of some <a href="https://en.wikipedia.org/wiki/Roko%27s_basilisk" target="_blank" rel="noreferrer noopener">Roko’s basilisk</a> level of power and intentionality, but because of the power and intentionality of its creators, real live humans who are moral agents by virtue of being humans. They are the ones who bear the responsibility here, and they are the ones who can suffer the consequences.</p></div><div><h2>Stay in touch</h2><p>Sign up for our free newsletter</p></div><div><h2><span>More from Aftermath</span></h2><div><div><a tabindex="0" href="https://aftermath.site/grimwild-backerkit-ttrpg-rpg-designer-missing"><h3>RPG Designer Goes Missing After Crowdfunding Award-Winning Game</h3></a><p>'Fulfilment of the project appears to be on hold indefinitely until Maxwell can be located'</p></div><div><a tabindex="0" href="https://aftermath.site/aftermath-hours-podcast-silksong-gta-vi-elder-scrolls-vi"><h3>What’s Going To Be The Next Silksong?</h3></a><p>"Given that Silksong is real and about to come out, what other super long-awaited games are left?"</p></div><div><a tabindex="0" href="https://aftermath.site/silksong-indie-delays"><h3>Indie Devs Are Delaying Their Games Because Of Silksong [Updated]</h3></a><p>'I feel like a little krill trying to not get eaten by a blue whale,' wrote one developer</p></div></div><a tabindex="0" href="https://aftermath.site/all">See all posts<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="#000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h13m-6-7 7 7-7 7"></path></svg></a></div></div><br/><br/>https://aftermath.site/ai-suffering-chatgpt]]></content:encoded>
        </item>
    </channel>
</rss>