<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - aftermath_site</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/aftermath_site.xml</link>
        <description>A cleaned-up version of the original gaming feed for aftermath_site</description>
        <lastBuildDate>Wed, 23 Apr 2025 18:26:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[AI Has Come For The Horse In The Hospital]]></title>
            <link>https://aftermath.site/google-ai-idioms</link>
            <guid>https://www.resetera.com/threads/ai-has-come-for-the-horse-in-the-hospital.1171704/</guid>
            <content:encoded><![CDATA[https://aftermath.site/google-ai-idioms<br/><br/><div id="readability-page-1" class="page"><div><p>This morning, my Bluesky feed was full of people who have discovered what seems to be a quirk in <a href="https://aftermath.site/google-ai-search-god-no-why" target="_blank" rel="noreferrer noopener">Google’s AI search</a>. Apparently, if you ask it the meaning of an idiom you’ve made up, it will treat it like a real, well-known expression and, with the embarrassing confidence only AI can muster, tell you what it means.</p><p>In one apt <a href="https://bsky.app/profile/albertburneko.bsky.social/post/3lni7smjfps2v" target="_blank" rel="noreferrer noopener">example</a>, noted language genius and Defector writer Albert Burneko asked Google to define “ask a six-headed mouse, get a three-legged stool,” which Google says “suggests asking the wrong question or seeking the wrong advice from someone unqualified can lead to a nonsensical or unhelpful response.” Other examples have included “<a href="https://bsky.app/profile/gregjenner.bsky.social/post/3lnhxkdywzc2m" target="_blank" rel="noreferrer noopener">you can’t lick a badger twice</a>, “<a href="https://bsky.app/profile/timmarchman.bsky.social/post/3lni72oq2t22m" target="_blank" rel="noreferrer noopener">you can’t make love to a dead dog</a>,” and other <a href="https://bsky.app/profile/gregjenner.bsky.social/post/3lnhxkdywzc2m/quotes" target="_blank" rel="noreferrer noopener">fake phrases</a> like “two cars short of a Winnebago,” “don’t touch my mother’s goats,” and “one sandwich in the stomach is worth two in the toilet.” In every instance, Google inserted literal nonsense into the lexicon.&nbsp;&nbsp;</p><p>This is funny because it’s such a clear highlight of one of AI’s most dangerous flaws: the confidence with which it makes things up and then presents them to users as true. Like a man you wish you weren’t talking to in a bar, it (or, more correctly, the people behind it) is desperate to come off as knowledgeable and authoritative, even when it can’t possibly be. It takes obvious bullshit and spins it into truth, in the same way it tells you to <a href="https://www.abc.net.au/listen/programs/pm/artificial-intelligence-tells-google-users-to-eat-rocks/103891826" target="_blank" rel="noreferrer noopener">eat rocks</a> or recently gave me <a href="https://bsky.app/profile/rcmacleod.bsky.social/post/3ll5kmvcqrk2x" target="_blank" rel="noreferrer noopener">physically impossible chicken</a>.</p><p>But I think making up meanings for invented idioms is particularly telling of one of AI’s biggest flaws. It made me think of comedian John Mulaney’s famous “horse in a hospital” joke:</p><div><figure><p><iframe title="There's a Horse In The Hospital | John Mulaney | Netflix Is A Joke" width="710" height="399" src="https://www.youtube.com/embed/JhkZMxgPxXU?feature=oembed" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe></p></figure></div><p>To ruin the joke for you with editor brain: The bit, a metaphor for the first Trump administration, is such a good use of language–besides the alliteration of “horse” and “hospital,” the phrase itself is evocatively weird. You assume it’s a people hospital (maybe because we usually say “vet” for animal hospitals), which raises all kinds of questions about how the horse got in there and what it would do in this situation, which provides the grounding for the rest of the joke. The two specific nouns paint a richer picture than, say, “there’s a bird in the bank;” they give high stakes, but it’s such a funny idea that it’s more intriguing than upsetting, providing just the right level of alarm to help you process your actual alarm about the real situation it calls back to.&nbsp;</p><p>If you hadn’t heard the joke and someone just said the phrase “there’s a horse in the hospital,” you’d have a sense of what it was getting at, but you couldn’t, or wouldn’t, define each word the way Google’s AI does. The joke and its reception are both so excellently human; the phrase has a meaning, but it isn’t a strict one-to-one. It highlights the fun of idioms and the things that only people can do with words. That’s given it some sticking power not just as a funny bit, but, to an extent, as part of our language.&nbsp;</p><p>So of course here’s AI to ruin that whole idea. AI can’t be clever or understand human cleverness; it can’t <em>play</em>; it can’t mess around with language, no matter what Sam Altman <a href="https://www.theguardian.com/books/2025/mar/12/a-machine-shaped-hand-read-a-story-from-openais-new-creative-writing-model" target="_blank" rel="noreferrer noopener">insists</a>. When you feed Google’s AI nonsense, it can’t screw around with that nonsense with you, it can’t go with the joke. Its answers aren’t funny because it’s creating and interpreting with you; they’re funny because of how energetically AI misses the point–not just of your question, but the whole idea of fun with language.</p><p>Beyond reminding us how easily AI spreads misinformation, this gives a glimpse of what everything we do could look like if the <a href="https://www.washingtonpost.com/education/2025/04/22/ai-schools-executive-order-trump-draft/?pwapi_token=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJyZWFzb24iOiJnaWZ0IiwibmJmIjoxNzQ1Mjk0NDAwLCJpc3MiOiJzdWJzY3JpcHRpb25zIiwiZXhwIjoxNzQ2Njc2Nzk5LCJpYXQiOjE3NDUyOTQ0MDAsImp0aSI6ImMyMmFlNWY1LWZlZTUtNGFkNS04YzI5LTRkMGY2NGM2MDJkMyIsInVybCI6Imh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS9lZHVjYXRpb24vMjAyNS8wNC8yMi9haS1zY2hvb2xzLWV4ZWN1dGl2ZS1vcmRlci10cnVtcC1kcmFmdC8ifQ.kZQoEmtK8YUZQVDTZidEQKeAAeZXTwtL7kFYsV8f0WA" target="_blank" rel="noreferrer noopener">losers</a> have their <a href="https://www.theverge.com/ai-artificial-intelligence/654223/cheat-on-everything-ai" target="_blank" rel="noreferrer noopener">way</a>. Mess and creativity are a <a href="https://www.theverge.com/ai-artificial-intelligence/654223/cheat-on-everything-ai" target="_blank" rel="noreferrer noopener">waste of time</a> to them; words and <a href="https://aftermath.site/let-humans-not-ai-be-bad-at-music" target="_blank" rel="noreferrer noopener">music</a> and <a href="https://aftermath.site/studio-ghibli-ai-art-openai-gpt-sam-altman-is-just-the-biggest-pile-of-shit" target="_blank" rel="noreferrer noopener">drawings</a> are just containers for inputs and outputs, boxes with no room for anything that makes these things meaningful or fun. This is the embarrassing future the “AI in everything” crowd wants; this is the machine they want us to find essential when, even in a low-stakes situation, it can only demonstrate that it has no use. As a person who makes their living through words, I’m all in favor of you pushing your weird phrases into conversation with your friends; do cool new things with language, because AI certainly can’t.</p></div><div><h2>Stay in touch</h2><p>Sign up for our free newsletter</p></div><div><h2><span>More from Aftermath</span></h2><div><a tabindex="0" href="https://aftermath.site/blue-prince-real-ending"><h3>Does Blue Prince Never End?</h3></a><p>After finding the 46th room, the journey in Blue Prince really begins. But does anyone actually know where it ends?</p></div><a tabindex="0" href="https://aftermath.site/all">See all posts<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="#000" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M5 12h13m-6-7 7 7-7 7"></path></svg></a></div></div>]]></content:encoded>
        </item>
    </channel>
</rss>