<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - gamedeveloper_com</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/gamedeveloper_com.xml</link>
        <description>A cleaned-up version of the original gaming feed for gamedeveloper_com</description>
        <lastBuildDate>Thu, 05 Feb 2026 18:18:42 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Deep Dive: Rethinking VR interaction design through hand tracking in Dimensional Double Shift]]></title>
            <link>https://www.gamedeveloper.com/extended-reality/deep-dive-rethinking-vr-interaction-design-through-hand-tracking-in-dimensional-double-shift</link>
            <guid>1427230</guid>
            <content:encoded><![CDATA[<div id="readability-page-1" class="page"><div data-module="content" data-testid="article-base-body-content"><p data-testid="content-paragraph"><span data-testid="content-text"><span>Game Developer Deep Dives are an ongoing series with the goal of shedding light on specific design, art, or technical features within a video game in order to show how seemingly simple, fundamental design decisions aren’t really that simple at all.</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text"><span>Earlier installments cover topics such as cover topics such as </span></span><span data-testid="content-text"><a target="_self" href="https://www.gamedeveloper.com/audio/deep-dive-sound-design-for-the-living-world-in-avatar-frontiers-of-pandora">the sound design of Avatar: Frontiers of Pandora,</a></span><span data-testid="content-text"><span> </span></span><span data-testid="content-text"><a target="_self" href="https://www.gamedeveloper.com/design/deep-dive-the-movement-of-echo-point-nova">how camera effects, sound FX, and VFX created a smooth and high octane movement system in&nbsp;Echo Point Nova</a></span><span data-testid="content-text"><span>,&nbsp;and the </span></span><span data-testid="content-text"><a target="_self" href="https://www.gamedeveloper.com/production/postmortem-bringing-the-cycle-frontier-to-unreal-editor-for-fortnite">technical process behind bringing&nbsp;The Cycle: Frontier&nbsp;to Unreal Editor for Fortnite</a></span><span data-testid="content-text"><span>.</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">For nearly a decade, Owlchemy Labs has explored what it means to touch the virtual world. From Cosmonius High's alien high hijinx, to </span><span data-testid="content-text"><span>Job Simulator's</span></span><span data-testid="content-text"> physical comedy, to </span><span data-testid="content-text"><span>Vacation Simulator's</span></span><span data-testid="content-text"> playful expressiveness, every game advanced our studio's mission: make VR interactions so natural that players forget the interface entirely.</span></p><p data-testid="content-paragraph"><span data-testid="content-text"><a target="_blank" href="https://dimensionaldoubleshift.com/">Dimensional Double Shift</a></span><span data-testid="content-text"><span> (DDS)</span></span><span data-testid="content-text"> represents the next step in that evolution: a full commitment to hand tracking. The result is a new design language grounded in embodiment, iteration, and accessibility.</span></p><p data-component="related-article"><span data-testid="related-article-title">Related:</span><a href="https://www.gamedeveloper.com/extended-reality/meta-shutters-three-vr-studios-as-part-of-reality-labs-layoffs" target="_self" data-discover="true">Meta shutters three VR studios as part of Reality Labs layoffs</a></p><p data-testid="content-paragraph"><span data-testid="content-text">"Moving from a controller, where people are used to pressing A or B or pulling a trigger, introduces a lot of new factors," said Alex Covert, Lead Gameplay Engineer. "Every time we design a new appliance for a dimension, we ask: How will the player actually grab this?"</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Expert System Engineer, Marc Huet, describes this shift as both philosophical and technical. "Our goal was to make hands feel present and believable—to minimize dissonance between what you </span><span data-testid="content-text"><span>see</span></span><span data-testid="content-text"> and what you </span><span data-testid="content-text"><span>feel,</span></span><span data-testid="content-text">" he explained. "That meant always keeping hands visible, supporting many grip types, and ensuring the virtual hand never broke immersion."</span></p><h2 data-testid="content-text" id="The Challenge">The Challenge</h2><p data-testid="content-paragraph"><span data-testid="content-text"><span>1. Lessons Learned from Early Failures</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">Early prototypes exposed the hidden complexity of human motion. "We had issues with hand sizing," Covert explained. "People felt uncomfortable using a giant hand that didn't match their own."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">The fix wasn't a player-facing slider. The system now scales the virtual hands to match the player's tracked hand size automatically—one of those invisible changes that quietly removes discomfort and makes everything feel more ‘you.' Small change, big comfort wins. It became a shorthand lesson internally: the best UX improvements are often the ones nobody notices because nothing feels wrong anymore.&nbsp;</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Sometimes, the obstacles weren't ergonomic but systemic. "We realized that a sock-puppet prototype conflicted with Meta's hand-tracking system," Covert recalled.&nbsp;</span></p><p data-component="related-article"><span data-testid="related-article-title">Related:</span><a href="https://www.gamedeveloper.com/extended-reality/former-meta-employees-allege-company-squashed-research-into-vr-child-abuse" target="_self" data-discover="true">Former Meta employees allege company squashed research into VR child abuse</a></p><p data-testid="content-paragraph"><span data-testid="content-text">The problem was less technical than behavioral. The first thing most people do with a puppet is talk to themselves. In practice, that motion turned out to be identical to Meta's system menu gesture. Instead of animating a character, players were repeatedly summoning the OS.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">There was no clever workaround. The gesture belonged to the platform. So the team didn't redesign the interaction — they cut puppets.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet's early experimentation echoed those constraints. "At first, our grab detection was binary—you were either grabbing or not," he said. "It worked for </span><span data-testid="content-text"><span>Vacation Simulator,</span></span><span data-testid="content-text"> but it didn't feel alive. We wanted to see your fingers react, your hands </span><span data-testid="content-text"><span>exist</span></span><span data-testid="content-text"> in the world."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">The team's first step toward that goal was rebuilding their "grab logic" from the ground up. Huet described the process as discovering just how unpredictable real human motion is: "There are so many valid ways to grab something—we had to decide which of those looked </span><span data-testid="content-text"><span>right</span></span><span data-testid="content-text"> in VR."</span></p><p>Image via Owlchemy Labs/Google</p><p data-testid="content-paragraph"><span data-testid="content-text"><span>2. Natural vs. Intuitive</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">Through hundreds of hours of playtesting, the team learned that natural and intuitive don't always align.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"Every person does it differently," Covert said. "Some pinch, some grab. The closer we get to how people actually use their hands, the less we have to explain."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet's research helped formalize that variability. Drawing from human-factors research on everyday grasping—such as a </span><span data-testid="content-text"><a target="_blank" href="https://ieeexplore.ieee.org/document/7243327">2016 IEEE study</a></span><span data-testid="content-text"> by robotics researchers Tomáš Feix and colleagues that categorizes common human grips based on form and intent—Huet built a library of grab techniques for VR. This includes grips like </span><span data-testid="content-text"><span>corner, clump, cylinder, hilt,</span></span><span data-testid="content-text"> and </span><span data-testid="content-text"><span>wand</span></span><span data-testid="content-text">—each mapped to real-world use.&nbsp;</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"We looked at how people hold cups, phones, and tools," he explained. "Every grip has a story—a context—and we wanted that reflected in VR." He also coined internal terms like "closedness," a 0-to-1 measure of how open or closed the player's hand is to define when a grab occurs.&nbsp;</span></p><p>Image via Owlchemy Labs/Google</p><p data-testid="content-paragraph"><span data-testid="content-text">"Some players over-grab, some pose-match," Huet said. "You have to balance both so things don't feel sticky or slippery. It's subtle math that ends up shaping </span><span data-testid="content-text"><span>how</span></span><span data-testid="content-text"> people play."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">This systematization allowed the team to move beyond imitation toward consistency. "It's fascinating research," Covert noted. "Marc showed pictures of different hand poses people use when grabbing things — holding a phone versus holding a cup. It grounded our approach."</span></p><p>Image via Owlchemy Labs/Google</p><h2 data-testid="content-text" id="The Approach">The Approach</h2><p data-testid="content-paragraph"><span data-testid="content-text"><span>1. Designing for Accessibility Through Simplicity</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">Removing controllers didn't just change the interface — it opened the door for more inclusive play.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"One of our big considerations is one-handed players," said Covert. "At one point, we had a two-handed twist-style pepper shaker, but that created accessibility concerns. So we added a shake alternative."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">This philosophy carried through every mechanic. </span><span data-testid="content-text"><span>DDS </span></span><span data-testid="content-text">introduced "snap points" for setting items down easily, freeing a player's hand mid-task.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"In the diner, the squeeze bottles have a threshold for how hard you need to squeeze," said Emma Atkinson, Technical Designer. "But if that's difficult, you can just tilt them and they'll pour out."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Moving to hand tracking also meant removing unnecessary instruction. Instead of teaching players new button metaphors or gesture rules, the team focused on subtracting friction and trusting existing human intuition. Because people already know how to use their hands, many interactions could be learned simply by doing—without prompts, overlays, or step-by-step tutorials.&nbsp;</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet extended that philosophy to invisible design systems. "Accessibility isn't always about menus or toggles," he said. "Sometimes it's about thresholds—if a player's range of motion is limited, we can scale the sensitivity so a smaller gesture still counts. The goal is to let everyone feel capable in VR."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">That sensitivity scaling, originally built for testing, became a subtle yet powerful accessibility feature. "You don't have to announce accessibility," Huet added. "You can just design it in."</span></p><p data-testid="content-paragraph"><span data-testid="content-text"><span>2. Creating Self-Haptics</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">Without vibration motors or triggers, the team reimagined tactile feedback from the ground up.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"The squishable UI buttons and keyboards give tactile feedback because your fingers touch each other," Covert explained. "That's what we call self-haptics. You feel yourself performing the action instead of relying on vibration."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Atkinson added, "You don't want your hand to become the object—you want to hold it. We try to minimize cognitive dissonance between what you see and what you feel."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet built on that idea technically, integrating collision physics that made contact feel grounded. "The raw hand data comes straight from the device," he explained. "We then run physics updates to see where your virtual hand </span><span data-testid="content-text"><span>should</span></span><span data-testid="content-text"> be after impact. There's a little tolerance—enough to give you that ‘hit' feeling before it breaks through."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">He described it as "letting physics fake haptics." "You're not feeling a vibration," he said, "but when the virtual hand resists or stops, your brain fills in the feedback."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">That philosophy—turning embodiment itself into feedback—became core to Owlchemy's design lexicon, alongside "bubble pass," the intuitive hand-to-hand object transfer system.</span></p><p data-testid="content-paragraph"><span data-testid="content-text"><span>3. Designing for Shared Gestures</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">Early builds tried to make passing explicit: both players had to perform a synchronized ‘handoff' gesture. The team replaced it with what became the ‘bubble pass'—an idea from Tim Winsky, implemented by Systems Engineer Marc Huet—where a thrown item hovers in front of the other player long enough to be grabbed. The result is more playful, and it teaches itself.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Even during internal playtests, people discovered the new pass system and had that moment of joy when they figured it out on their own.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">The change was data-driven. "We added analytics to see how often players used the old passing system and found that almost no one was using it," Covert said. "That told us it was time to rethink the design."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet's grab system played an unseen role here too. "When you toss something, timing matters; milliseconds affect how real it feels," he said. "So we built in what we call ‘sticky' and ‘slippery' release thresholds. It's how you tell the difference between handing off a mug and throwing a beach ball."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">His experiments revealed that velocity and openness need to respond to intent. "If you open your hand slowly, the object should linger. If you open fast, it should fly," Huet explained. "Those little cues make shared gestures feel believable." It also learns what ‘I'm ready' looks like: hand up, waiting. When it sees that, it helps the object meet the hand—so catching feels intentional instead of accidental.</span></p><p data-testid="content-paragraph"><span data-testid="content-text"><span>4. Designing Around Technical Limits</span></span></p><p data-testid="content-paragraph"><span data-testid="content-text">Hardware realities still present creative challenges. "When you point away from yourself with the sprayer, your hand can block the headset cameras, so it doesn't always detect the motion," Covert explained.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Instead of seeing such limitations as setbacks, the team treats them as opportunities for innovation. "We've even talked about adding aerosol spray cans where you press the top with your index finger," Covert added. "We don't have support for that yet, but it's on the wishlist."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet elaborated on those constraints: "Hand occlusion, lighting, tracking speed—those are constant battles," he said. "If a hand moves too fast or leaves the camera's field of view, tracking breaks. So we design around that by keeping actions close, within this invisible box in front of you."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Environmental conditions also shape interaction design. Hand tracking behaves differently depending on lighting, camera visibility, and the underlying capabilities of the headset itself. In low-light or high-occlusion situations, tracking data can become noisy.&nbsp;</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Why?&nbsp;</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Well, not all systems compensate for that noise in the same way. Some systems include infrared illuminators to support hand tracking in low-light conditions, while others degrade faster as lighting drops.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Rather than treating those moments as failure states, the team designed interactions to be forgiving by default: dropped objects recover, missed grabs self-correct, and small errors never cascade into frustration. The guiding principle was simple: Players should never feel punished by physics or hardware constraints.</span></p><h2 data-testid="content-text" id="The Results">The Results</h2><p data-testid="content-paragraph"><span data-testid="content-text">Hand tracking didn't just expand accessibility. It made play feel human.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Players naturally gestured, waved, and improvised together. They learned by doing, not by reading.&nbsp;</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"We almost don't need to design new ways of doing things," Covert said. "The closer we get to how people actually use their hands, the more natural the game feels."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Huet added, "Our success is when people stop noticing the system. When your virtual hand feels like </span><span data-testid="content-text"><span>your</span></span><span data-testid="content-text"> hand—when you don't think about grabbing, you just grab—that's when VR disappears and embodiment takes over."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">Across development, the team's iterative design language. Terms like </span><span data-testid="content-text"><span>self-haptics</span></span><span data-testid="content-text"> and </span><span data-testid="content-text"><span>bubble pass became</span></span><span data-testid="content-text"> shorthand for a culture of experimentation and discovery. "You could almost make a dictionary of the words we've made up," Atkinson laughed. "It's our own language."</span></p><p data-testid="content-paragraph"><span data-testid="content-text">For Owlchemy Labs, hand tracking reaffirmed its core philosophy: interactions should be instinctive, inclusive, and joyful.</span></p><p data-testid="content-paragraph"><span data-testid="content-text">"We're still improving our authoring tools," Huet said. "But every iteration teaches us something new about how humans move—and how to make VR move with them."</span></p><p>Image via Owlchemy Labs/Google</p><p data-testid="content-paragraph"><span data-testid="content-text">Key Takeaways</span></p><div data-component="basic-list"><ul data-testid="basic-list-unordered"><li><div><p data-testid="content-paragraph"><span data-testid="content-text"><span>Hand tracking is both interface and embodiment.</span></span><span data-testid="content-text"> Designing gestures means understanding how people feel feedback: not just how they perform it.</span></p></div></li><li><div><p data-testid="content-paragraph"><span data-testid="content-text"><span>Authoring meets procedural motion. </span></span><span data-testid="content-text">Strong hand-tracking systems allow developers to lock poses when precision matters (i.e., picking up a cup one way) while still allowing motion to adapt to how players actually reach, slide, and settle into a grab. The mix of authorship and procedurality is a key aspect of our system many developers miss. They try to hard to do one or the other</span></p></div></li><li><div><p data-testid="content-paragraph"><span data-testid="content-text"><span>Accessibility improves when interaction simplifies</span></span><span data-testid="content-text">. Removing hardware often clarifies rather than complicates the experience.</span></p></div></li><li><div><p data-testid="content-paragraph"><span data-testid="content-text"><span>Iteration is the constant.</span></span><span data-testid="content-text"> Every friction point (hand size, gesture mismatch, or camera occlusion) drives better design.</span></p></div></li><li><div><p data-testid="content-paragraph"><span data-testid="content-text"><span>Analytics close the loop. </span></span><span data-testid="content-text">Observation, data, and playtesting inform every refinement.</span></p></div></li><li><div><p data-testid="content-paragraph"><span data-testid="content-text"><span>VR's future is hands-on</span></span><span data-testid="content-text">. Interaction and embodiment are no longer separate disciplines. They're one and the same.</span></p></div></li></ul></div></div></div><br/><br/>https://www.gamedeveloper.com/extended-reality/deep-dive-rethinking-vr-interaction-design-through-hand-tracking-in-dimensional-double-shift]]></content:encoded>
            <dc:creator>invalid@example.com (ResetEra_Newsbot</dc:creator>
        </item>
    </channel>
</rss>