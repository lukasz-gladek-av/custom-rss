<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - arstechnica_com</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/arstechnica_com.xml</link>
        <description>A cleaned-up version of the original gaming feed for arstechnica_com</description>
        <lastBuildDate>Thu, 16 Jan 2025 15:44:06 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Intel Arc B570 review: At $219, the cheapest good graphics card]]></title>
            <link>https://arstechnica.com/gadgets/2025/01/intel-arc-b570-review-at-219-the-cheapest-good-graphics-card/</link>
            <guid>https://www.resetera.com/threads/intel-arc-b570-review-at-219-the-cheapest-good-graphics-card.1084077/</guid>
            <content:encoded><![CDATA[https://arstechnica.com/gadgets/2025/01/intel-arc-b570-review-at-219-the-cheapest-good-graphics-card/<br/><br/><div id="readability-page-1" class="page"><div id="main">
            <article data-id="2069802">
  
  <header>
  <div>
    <div>
      <p><span>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="section-gadgets_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath><clipPath id="section-gadgets_svg__b"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g clip-path="url(#section-gadgets_svg__a)"><g fill="currentColor" clip-path="url(#section-gadgets_svg__b)"><path d="M38 22c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2V4h-4V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2s-2 .9-2 2v2h-6V2c0-1.1-.9-2-2-2S8 .9 8 2v2H4v4H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v6H2c-1.1 0-2 .9-2 2s.9 2 2 2h2v4h4v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h6v2c0 1.1.9 2 2 2s2-.9 2-2v-2h4v-4h2c1.1 0 2-.9 2-2s-.9-2-2-2h-2v-6zm-6 10H8V8h24z"></path><path d="M24.7 17.3 20 12h-7.1c-.6 0-1 .4-1 1s.4 1 1 1h6.3l4.1 4.7L20 22h8v-8z"></path><path d="m15.2 22.7 4.7 5.3H27c.6 0 1-.4 1-1s-.4-1-1-1h-6.3l-4.1-4.7 3.3-3.3h-8v8z"></path></g></g></svg>
  </span>
  <span>
    minimum viable graphics card
  </span>
</p>
    </div>

    

    <p>
      Competitive with a 4060, with 10GB RAM and a lower price (if you can get one).
    </p>

    

    <div id="caption-2071172">
    
    <p>
      The ASRock Challenger version of Intel's Arc B570 GPU.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>

    <div>
    
    <p>
      The ASRock Challenger version of Intel's Arc B570 GPU.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p><a href="https://arstechnica.com/gadgets/2024/12/review-intel-arc-b580-is-a-compelling-if-incredibly-tardy-250-midrange-gpu/">Intel's Arc B580 graphics cards</a> have been its best-reviewed to date, maintaining the aggressive pricing of the old A-series Arc cards with fewer driver bugs, fewer weird performance outliers, and fewer caveats all around.</p>
<p>And this appears to be translating to retail success—the B580 is sold out across the board and difficult to find at its $249 MSRP. It's hard to tell if this is because demand has been good or supply was low (Intel says it has been restocking "weekly," for what that's worth). But regardless, there's clearly been some pent-up demand for an inexpensive-but-competent entry-level graphics card with decent ray-tracing performance and power efficiency and more than 8GB of RAM.</p>
<p>The Arc B570 is a less-powerful, less-interesting card than the B580, with fewer of Intel's Xe-cores, less memory bandwidth, and 10GB of RAM instead of 12GB. But it offers performance very similar to the RTX 4060 for $80 less—at least, if Intel and its partners can keep it in stock at that price—which makes it a dramatically more interesting budget option than $200-ish cards like the GeForce RTX 3050 or Radeon RX 6600.</p>
<h2>An Arc B580, but less</h2>
<figure>
    
          <figcaption>
        <div>
    
    <p>
      Everything about the B570 is scaled down just a bit compared to the B580.

              <span>
          Credit:

          
          Intel

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The B570 uses the exact same GPU silicon as the B580 but with a slightly lower clock speed and a couple of Xe cores disabled. Memory bus width has also been nudged downward, from 192-bits to 160-bits. The downgrades also give it a lower total board power of 150 W—quite a bit less than the 190 W of the B580, but still enough to require a single 8-pin power connector from your power supply rather than being able to get all of its power from your PCI Express slot.</p>
<p>The card's 10GB of RAM just barely allows Intel to claim that the B570 is more future-proof than cards like the RTX 4060, which still ship with 8GB of RAM. Though 8GB won't limit your performance much at the resolutions that the B570 is powerful enough to render, a little more memory can help in a handful of edge cases, even if it's only 2GB.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          


<p>Like the B580, the B570 ships with an eight-lane PCIe 4.0 interface, and Intel's GPUs rely disproportionately on the <a href="https://arstechnica.com/gadgets/2021/09/new-drivers-add-performance-boosting-memory-access-feature-to-older-amd-gpus/">Resizable BAR feature</a> to perform optimally (also sometimes called ReBar, Smart Access Memory, or SAM, depending on your motherboard). Neither of these factors is a big deal if you have a PC made within the last five years or so, but it does make Arc a worse fit if you're trying to upgrade an older system that uses PCIe 3.0 for the graphics slot or which doesn't support ReBAR. Just something to be aware of if you're trying to extend the life of an older desktop with integrated graphics.</p>

<h2>Performance and power</h2>
<p>On paper, you'd expect the B570 to run between 80 and 90 percent as fast as a fully enabled B580, and in our testing, that's almost exactly what you get.</p>


<p>The B570 hovers at right around the same performance level as the RTX 4060 and the 8GB version of the Radeon RX 7600 in our tests—sometimes a little faster, sometimes a little slower, but never by more than a few frames per second in any direction. It's also faster than the old Arc A750 and nearly as fast as the A770, with none of the game-specific performance dips (observe the <em>Assassin's Creed</em> titles in particular).</p>
<p>Like the B580, it also keeps pace with Nvidia's performance in games with heavy ray-tracing effects, taking roughly the same-sized performance hit as the RTX 4060 (though as a low-end card, even at 1080p you usually won't get stable 30 or 60 fps rates in most ray-traced games without a little boost from DLSS/XeSS/FSR upscaling). This means it blows both the 8GB and 16GB Radeon RX 7600-series cards out of the water in games with heavy ray-tracing effects.</p>


<p>In these games at these resolutions—and again, we're sticking to 1080p and 1440p, the resolutions this card will reasonably be able to hit—we don't measure a <em>ton</em> of extra performance that we can attribute specifically to the difference between 8GB of RAM and 10GB of RAM.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>We do see a hint of extra improvement in <em>Forza Horizon 5</em>, a game where the 8GB Radeon RX 7600 and the 8GB Intel Arc A750 do struggle compared to similar cards with a bit more RAM (Nvidia's 8GB cards seem to handle it fine, though). An extra 2GB of RAM isn't a ton, but it may be enough to make the difference with games that teeter right on the edge of being RAM-limited at 8GB. But as we saw with the 16GB RX 7600 XT, the majority of the time, the power of the GPU itself will still end up being the limiting factor.</p>
<figure>
    
      </figure>

<p>Finally, the B570's power efficiency when gaming is downright impressive, consuming around 30 W less than the B580, a little bit less than the RTX 4060, and a lot less than the RX 7600 (with the caveat that we are using software-reported power consumption figures, which aren't as accurate as the power usage measured with specialized hardware, and comparisons between different manufacturers and different GPU architectures are imperfect). It seems safe to say that Intel's Battlemage cards are quite competitive on power efficiency when they're playing games, something that wasn't really true of the A700-series GPUs.</p>
<p>It's worth noting, <a href="https://www.techpowerup.com/review/intel-arc-b580/38.html">as other reviewers have pointed out</a>, that Arc's&nbsp;<em>idle</em> power consumption is still fairly high compared to modern AMD and Nvidia cards, especially with multiple monitors connected.</p>

<h2>A competent Nvidia alternative if you can’t get a B580 for $249</h2>
<figure>
    
          <figcaption>
        <div>
    
    <p>
      Intel's Arc is making some headway, at least on performance and specs.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Buying an Intel card still comes with some caveats, even if you're only spending $220. Nvidia's GPUs remain dominant and well-supported, and buying anything from any other company means you lose out on DLSS upscaling and the wider universe of AI and rendering software that takes specific advantage of CUDA or other Nvidia GPU features.</p>
<p>I've also got to wonder how devoted Intel will be to its graphics cards in the long term—Intel's Michelle Johnston Holthaus said that the company remained "very committed to the discrete graphics market," though that's small comfort coming from a company's interim co-CEO.</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>I've got to imagine that when Intel <a href="https://arstechnica.com/gadgets/2017/11/intel-poaches-amds-top-gpu-architect-to-build-its-own-discrete-graphics-chips/">set itself to the task of developing dedicated graphics cards</a> many years and several CEOs ago, it was not the company's intent to produce exclusively sub-$300 1080p graphics cards. But so far, this incredibly competitive, low-margin, and price-sensitive market segment is the only one the company seems to be able to contest. Intel continues to rapidly shed or spin off side projects like the NUC, its series of cryptocurrency mining processors, the RealSense IR cameras, and Altera's FPGA business. Betting on the long-term health of the Arc GPU business still seems risky.</p>
<p>That said, I do like the B570 and B580 quite a bit, at least if you can get them at the prices Intel is advertising. Both are good alternatives to the dominant RTX 4060 that require fewer compromises than AMD's cards, and both demonstrate that Intel has learned from a lot of the mistakes and growing pains that were evident in the first-generation Arc cards.</p>
<p>For $30 extra, I'd still lean toward the B580 if you can find it at that price. The extra performance and extra RAM are worth paying a little more for. But if you're trying to assemble a decent gaming system for somewhere between $500 and $700, and if it stays as cheap as it's supposed to be, the B570 is a better answer to the RTX 4060 than anything AMD has been able to put out so far.</p>
<h3>The good</h3>
<ul>
<li>It's cheap if you can get it at MSRP.</li>
<li>Performs in the same ballpark as the GeForce RTX 4060 and Radeon RX 7600.</li>
<li>Includes the niceties you get when buying a modern GPU rather than an older one that has come down in price, including AV1 video encoding support and competent hardware-accelerated ray-tracing performance.</li>
<li>Intel's drivers, performance consistency, and power use under load have all come a long way since last generation.</li>
</ul>
<h3>The bad</h3>
<ul>
<li>Missing out on Nvidia-specific things like DLSS.</li>
<li>Arc cards still have higher power use at idle than competing GPUs.</li>
<li>Paying just $30 more for an Arc B580 (if you could find one) gets you a decent step up in all-around performance.</li>
</ul>
<h3>The ugly</h3>
<ul>
<li>The Arc B580 sold out fast, and when they're not at MSRP, these GPUs make less sense.</li>
</ul>


          
                  </div>

                  
          






  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/01/intel-arc-b570-review-at-219-the-cheapest-good-graphics-card/#comments" title="20 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    20 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  


  

  </div></div>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[It’s official: Take a first look at the Switch 2]]></title>
            <link>https://arstechnica.com/gaming/2025/01/its-official-take-a-first-look-at-the-switch-2/</link>
            <guid>https://www.resetera.com/threads/it%E2%80%99s-official-take-a-first-look-at-the-switch-2.1084074/</guid>
            <content:encoded><![CDATA[https://arstechnica.com/gaming/2025/01/its-official-take-a-first-look-at-the-switch-2/<br/><br/><div id="readability-page-1" class="page"><div>
                      
                      
          <figure><p><iframe allow="fullscreen" loading="lazy" src="https://www.youtube.com/embed/itpcsQQvgAQ?start=0&amp;wmode=transparent"></iframe></p></figure>
<p>After <a href="https://arstechnica.com/gaming/2021/09/the-thing-after-the-switch-how-overheard-chatter-led-to-a-nintendo-goose-chase/">months</a> and <a href="https://arstechnica.com/gaming/2023/09/nintendo-reportedly-showing-devs-switch-2-with-upscaled-breath-of-the-wild/">years</a> of <a href="https://arstechnica.com/gaming/2023/07/report-nintendos-next-console-ships-late-2024-still-supports-cartridges/">rumors</a> and <a href="https://arstechnica.com/gaming/2024/05/nintendo-pre-announces-a-switch-2-announcement-is-coming-eventually/">official hints</a>, Nintendo has finally pulled back the curtain on the Switch 2 with <a href="https://www.youtube.com/watch?v=itpcsQQvgAQ">a first-look trailer</a> (and <a href="https://www.nintendo.com/successor/en-us/index.html">sparse promo web site</a>) highlighting many small changes from the old Switch.</p>
<div>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="arrow-blocks-right_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g fill="currentColor" clip-path="url(#arrow-blocks-right_svg__a)"><path d="M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z"></path></g></svg>

    <p><span>The Switch 2 tablet, shown with original Switch Joy-Cons for scale.</span>
                    <span>
                      <a href="https://www.youtube.com/watch?v=itpcsQQvgAQ" target="_blank">Nintendo</a>
                  </span>
          </p>
  </div>

<p>The short trailer shows off the Switch 2's larger tablet and screen, and a slightly more rounded edge on the top and bottom. The new system also sports an additional USB-C port on the top (next to a headphone jack) and a wider, U-shaped kickstand along the backside that can support the system at a number of wider angles.</p>
<p>The trailer also shows off black Joy-Cons that are significantly larger than those on the original Switch, with colored accents behind the joystick itself. An extended, colored Joy-Con "rail" on the inner edge features wider shoulder buttons and a new connector in the center. Rather than sliding in vertically, like the plastic rail on the Switch Joy-Cons, the controllers on the Switch 2 snap in horizontally with <a href="https://arstechnica.com/gaming/2024/04/report-switch-2-joy-cons-will-attach-via-magnets/">what appears to be a magnetic connection</a>, and disconnects with the aid of a horizontal lever to the side of the shoulder buttons.</p>
<div>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 40 40"><defs><clipPath id="arrow-blocks-right_svg__a"><path fill="none" d="M0 0h40v40H0z"></path></clipPath></defs><g fill="currentColor" clip-path="url(#arrow-blocks-right_svg__a)"><path d="M32 16h8v8h-8zm-8 8h8v8h-8zm-8 8h8v8h-8zm8-24h8v8h-8zm-8-8h8v8h-8zM0 16h16v8H0z"></path></g></svg>

    <p><span>Close-up on the expanded "rail" on the inner edge of the Joy-Cons.</span>
                    <span>
                      <a href="https://www.youtube.com/watch?v=itpcsQQvgAQ" target="_blank">Nintendo</a>
                  </span>
          </p>
  </div>

<p>The right Joy-Con on the Switch 2 sports an additional small, square button below the Home button. Its function is currently unknown.</p>

          
                      
                  </div></div>]]></content:encoded>
        </item>
    </channel>
</rss>