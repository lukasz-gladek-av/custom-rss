<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>maGaming RSS Feed - arstechnica_com</title>
        <link>https://lukasz-gladek-av.github.io/custom-rss/arstechnica_com.xml</link>
        <description>A cleaned-up version of the original gaming feed for arstechnica_com</description>
        <lastBuildDate>Fri, 24 Jan 2025 19:37:23 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Nvidia GeForce RTX 5090 costs as much as a whole gaming PC—but it sure is fast]]></title>
            <link>https://arstechnica.com/gadgets/2025/01/review-nvidias-geforce-rtx-5090-is-the-first-gpu-that-can-beat-the-rtx-4090/</link>
            <guid>https://www.resetera.com/threads/nvidia-geforce-rtx-5090-costs-as-much-as-a-whole-gaming-pc%E2%80%94but-it-sure-is-fast.1091286/</guid>
            <content:encoded><![CDATA[https://arstechnica.com/gadgets/2025/01/review-nvidias-geforce-rtx-5090-is-the-first-gpu-that-can-beat-the-rtx-4090/<br/><br/><div id="readability-page-1" class="page"><div id="main">
            <article data-id="2071667">
  
  <header>
  <div>
    

    

    <p>
      Even setting aside Frame Generation, this is a fast, power-hungry $2,000 GPU.
    </p>

    

    <div id="caption-2072543">
    
    <p><span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>

    <div>
    
    <p><span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
  </div>
</header>


  

  
      
    
    <div>
                      
                      
          
<p>Nvidia's GeForce RTX 5090 starts at $1,999 before you factor in upsells from the company's partners or price increases driven by scalpers and/or genuine demand. It costs more than my entire gaming PC.</p>
<p>The new GPU is so expensive that you could build an entire well-specced gaming PC with Nvidia's next-fastest GPU in it—the $999 RTX 5080, which we don't have in hand yet—for the same money, or maybe even a little less with judicious component selection. It's not the most expensive GPU that Nvidia has ever launched—2018's $2,499 Titan RTX has it beat, and 2022's RTX 3090 Ti also cost $2,000—but it's safe to say it's not really a GPU intended for the masses.</p>
<p>At least as far as gaming is concerned, the 5090 is the very definition of a halo product; it's for people who demand the best and newest thing regardless of what it costs (the calculus is probably different for deep-pocketed people and companies who want to use them as some kind of generative AI accelerator). And on this front, at least, the 5090 is successful. It's the newest and fastest GPU you can buy, and the competition is not particularly close. It's also a showcase for DLSS Multi-Frame Generation, a new feature unique to the 50-series cards that Nvidia is leaning on heavily to make its new GPUs look better than they already are.</p>
<h2>Founders Edition cards: Design and cooling</h2>
<div><table>
<tbody>
<tr>
<th></th>
<th>RTX 5090</th>
<th>RTX 4090</th>
<th>RTX 5080</th>
<th>RTX 4080 Super</th>
</tr>
<tr>
<th>CUDA cores</th>
<td>21,760</td>
<td>16,384</td>
<td>10,752</td>
<td>10,240</td>
</tr>
<tr>
<th>Boost clock</th>
<td>2,410 MHz</td>
<td>2,520 MHz</td>
<td>2,617 MHz</td>
<td>2,550 MHz</td>
</tr>
<tr>
<th>Memory bus width</th>
<td>512-bit</td>
<td>384-bit</td>
<td>256-bit</td>
<td>256-bit</td>
</tr>
<tr>
<th>Memory bandwidth</th>
<td>1,792 GB/s</td>
<td>1,008 GB/s</td>
<td>960 GB/s</td>
<td>736 GB/s</td>
</tr>
<tr>
<th>Memory size</th>
<td>32GB GDDR7</td>
<td>24GB GDDR6X</td>
<td>16GB GDDR7</td>
<td>16GB GDDR6X</td>
</tr>
<tr>
<th>TGP</th>
<td>575 W</td>
<td>450 W</td>
<td>360 W</td>
<td>320 W</td>
</tr>
</tbody>
</table></div>
<p>We won’t spend too long talking about the specific designs of Nvidia’s Founders Edition cards since many buyers will experience the Blackwell GPUs with cards from Nvidia’s partners instead (the cards we’ve seen so far mostly look like the expected fare: gargantuan triple-slot triple-fan coolers, with varying degrees of RGB). But it's worth noting that Nvidia has addressed a couple of my functional gripes with the 4090/4080-series design.</p>

          
                      
                  </div>
                    
        
          
    
    <div>
          
          
<p>The first was the sheer dimensions of each card—not an issue unique to Nvidia, but one that frequently caused problems for me as someone who tends toward ITX-based PCs and smaller builds. The 5090 and 5080 FE designs are the same length and height as the 4090 and 4080 FE designs, but they only take up two slots instead of three, which will make them an easier fit for many cases.</p>
<p>Nvidia has also tweaked the cards’ 12VHPWR connector, recessing it into the card and mounting it at a slight angle instead of having it sticking straight out of the top edge. The height of the 4090/4080 FE design made some cases hard to close up once you factored in the additional height of a 12VHPWR cable or Nvidia’s many-tentacled 8-pin-to-12VHPWR adapter. The angled connector still extends a bit beyond the top of the card, but it’s easier to tuck the cable away so you can put the side back on your case.</p>


<p>Finally, Nvidia has changed its cooler—whereas most OEM GPUs mount all their fans on the top of the GPU, Nvidia has historically placed one fan on each side of the card. In a standard ATX case with the GPU mounted parallel to the bottom of the case, this wasn’t a <em>huge</em> deal—there’s plenty of room for that air to circulate inside the case and to be expelled by whatever case fans you have installed.</p>
<p>But in “sandwich-style” ITX cases, where a riser cable wraps around so the GPU can be mounted parallel to the motherboard, the fan on the bottom side of the GPU was poorly placed. In many sandwich-style cases, the GPU fan will dump heat against the back of the motherboard, making it harder to keep the GPU cool and creating heat problems elsewhere besides. The new GPUs mount both fans on the top of the cards.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>Nvidia’s Founders Edition cards <a href="https://www.google.com/search?q=geforce+founders+edition+heat+problems&amp;oq=geforce+founders+edition+heat+problems&amp;gs_lcrp=EgRlZGdlKgYIABBFGDkyBggAEEUYOdIBCDQxMzhqMGoxqAIAsAIA&amp;sourceid=chrome&amp;ie=UTF-8">have had heat issues in the past</a>—most notably the 30-series GPUs—and that was my first question going in. A smaller cooler plus a dramatically higher peak power draw seems like a recipe for overheating.</p>

<figure>
    
          <figcaption>
        <div>
    
    <p>
      Temperatures for the various cards we re-tested for this review. The 5090 FE is the toastiest of all of them, but it still has a safe operating temperature.

          </p>
  </div>
      </figcaption>
      </figure>

<p>At least for the 5090, the smaller cooler does mean higher temperatures—around 10 to 12 degrees Celsius higher when running the same benchmarks as the RTX 4090 Founders Edition. And while temperatures of around 77 degrees aren’t hugely concerning, this is sort of a best-case scenario, with an adequately cooled testbed case with the side panel totally removed and ambient temperatures at around 21° or 22° Celsius. You’ll just want to make sure you have a good amount of airflow in your case if you buy one of these.</p>
<h2>Testbed notes</h2>

<p>A new high-end Nvidia GPU is a good reason to tweak our test bed and suite of games, and we’ve done both here. Mainly, we added <a href="https://thermaltakeusa.com/products/toughpower-gf-a3-gold-1050w-tt-premium-edition-ps-tpd-1050fnfagu-l">a 1050 W Thermaltake Toughpower GF A3 power supply</a>—Nvidia recommends at least 1000 W for the 5090, and this one has a native 12VHPWR connector for convenience. We’ve also swapped the Ryzen 7 7800X3D for a slightly faster Ryzen 7 9800X3D to reduce the odds that the CPU will bottleneck performance as we try to hit high frame rates.</p>
<p>As for the suite of games, we’ve removed a couple of older titles and added some with built-in benchmarks that will tax these GPUs a bit more, especially at 4K with all the settings turned up. Those games include the RT Overdrive preset in the perennially punishing <em>Cyberpunk 2077 </em>and <em>Black Myth: Wukong</em> in Cinematic mode, both games where even the RTX 4090 struggles to hit 60 fps without an assist from DLSS. We’ve also added <em>Horizon Zero Dawn Remastered</em>, a recent release that doesn’t include ray-tracing effects but does support most DLSS 3 and FSR 3 features (including FSR Frame Generation).</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>We’ve tried to strike a balance between games with ray-tracing effects and games without it, though most AAA games these days include it, and modern GPUs should be able to handle it well (best of luck to AMD with its upcoming RDNA 4 cards).</p>
<p>For the 5090, we've run all tests in 4K—if you don't care about running games in 4K, even if you want super-high frame rates at 1440p or for some kind of ultrawide monitor, the 5090 is probably overkill. When we run upscaling tests, we use the newest DLSS version available for Nvidia cards, the newest FSR version available for AMD cards, and the newest XeSS version available for Intel cards (not relevant here, just stating for the record), and we use the "Quality" setting (at 4K, that equates to an actual rendering version of 1440p).</p>
<h2>Rendering performance: A lot faster, a lot more power-hungry</h2>
<p>Before we talk about Frame Generation or "fake frames," let's compare apples to apples and just examine the 5090's rendering performance.</p>
<p>The card mainly benefits from four things compared to the 4090: the updated Blackwell GPU architecture, a nearly 33 percent increase in the number of CUDA cores, an upgrade from GDDR6X to GDDR7, and a move from a 384-bit memory bus to a 512-bit bus. It also jumps from 24GB of RAM to 32GB, but games generally aren't butting up against a 24GB limit yet, so the capacity increase by itself shouldn't really change performance if all you're focused on is gaming.</p>



<p>And for people who prioritize performance over all else, the 5090 is a big deal—it's the first consumer graphics card from any company that is faster than a 4090, as Nvidia never spruced up the 4090 last year when it did its mid-generation Super refreshes of the 4080, 4070 Ti, and 4070.</p>
<p>Comparing natively rendered games at 4K, the 5090 is between 17 percent and 40 percent faster than the 4090, with most of the games we tested landing somewhere in the low to high 30 percent range. That's an undeniably big bump, one that's roughly commensurate with the increase in the number of CUDA cores. Tests run with DLSS enabled (both upscaling-only and with Frame Generation running in 2x mode) improve by roughly the same amount.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          


<p>You could find things to be disappointed about if you went looking for them. That 30-something-percent performance increase comes with a 35 percent increase in power use in our testing under load with punishing 4K games—the 4090 tops out around 420 W, whereas the 5090 went all the way up to 573 W, with the 5090 coming closer to its 575 W TDP than the 4090 does to its theoretical 450 W maximum. The 50-series cards use the same TSMC 4N manufacturing process as the 40-series cards, and increasing the number of transistors without changing the process results in a chip that uses more power (though it should be said that capping frame rates, running at lower resolutions, or running less-demanding games can rein in that power use a bit).</p>
<figure>
    
          <figcaption>
        <div>
    
    <p>
      Power draw under load goes up by an amount roughly commensurate with performance. The 4090 was already power-hungry; the 5090 is dramatically more so.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The 5090's 30-something percent increase over the 4090 might also seem underwhelming if you recall that the 4090 <a href="https://www.tomshardware.com/reviews/nvidia-geforce-rtx-4090-review/3">was around 55 percent faster</a> than the previous-generation 3090 Ti while consuming about the same amount of power. To be <em>even faster</em> than a 4090 is no small feat—AMD's fastest GPU is more in line with Nvidia's 4080 Super—but if you're comparing the two cards using the exact same tests, the relative leap is less seismic.</p>
<p>That brings us to Nvidia's answer for that problem: DLSS 4 and its Multi-Frame Generation feature.</p>

<h2>DLSS 4 and Multi-Frame Generation</h2>
<p>As a refresher, Nvidia's DLSS Frame Generation feature, as introduced in the GeForce 40-series, takes DLSS upscaling one step further. The upscaling feature inserted interpolated pixels into a rendered image to make it look like a sharper, higher-resolution image without having to do all the work of rendering all those pixels. DLSS FG would interpolate an entire frame between rendered frames, boosting your FPS without dramatically boosting the amount of work your GPU was doing. If you used DLSS upscaling and FG at the same time, Nvidia could claim that seven out of eight pixels on your screen were generated by AI.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>DLSS Multi-Frame Generation (hereafter MFG, for simplicity's sake) does the same thing, but it can generate one to three interpolated frames for every rendered frame. The marketing numbers have gone up, too; now, 15 out of every 16 pixels on your screen can be generated by AI.</p>
<figure>
    
          <figcaption>
        <div>
    
    <p>
      Nvidia might point to this and say that the 5090 is over twice as fast as the 4090, but that's not really comparing apples to apples. Expect this issue to persist over the lifetime of the 50-series.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>Nvidia provided reviewers with a preview build of <em>Cyberpunk 2077 </em>with DLSS MFG enabled, which gives us an example of how those settings will be exposed to users. For 40-series cards that only support the regular DLSS FG, you won't notice a difference in games that support MFG—Frame Generation is still just one toggle you can turn on or off. For 50-series cards that support MFG, you'll be able to choose from among a few options, just as you currently can with other DLSS quality settings.</p>
<p>The “2x” mode is the old version of DLSS FG and is supported by both the 50-series cards and 40-series GPUs; it promises one generated frame for every rendered frame (two frames total, hence "2x"). The “3x” and “4x” modes are new to the 50-series and promise two and three generated frames (respectively) for every rendered frame. Like the original DLSS FG, MFG can be used in concert with normal DLSS upscaling, or it can be used independently.</p>
<p>One problem with the original DLSS FG was latency—user input was only being sampled at the natively rendered frame rate, meaning you could be looking at 60 frames per second on your display but only having your input polled 30 times per second. Another is image quality; as good as the DLSS algorithms can be at guessing and recreating what a natively rendered pixel would look like, you'll inevitably see errors, particularly in fine details.</p>
<p>Both these problems contribute to the third problem with DLSS FG: Without a decent underlying frame rate, the lag you feel and the weird visual artifacts you notice will both be more pronounced. So DLSS FG can be useful for turning 120 fps into 240 fps, or even 60 fps into 120 fps. But it's not as helpful if you're trying to get from 20 or 30 fps up to a smooth 60 fps.</p>

          
                  </div>
                    
        
          
    
    <div>
          
          
<p>We'll be taking a closer look at the DLSS upgrades in the next couple of weeks (including MFG and the new transformer model, which will supposedly increase upscaling quality and supports all RTX GPUs). But in our limited testing so far, the issues with DLSS MFG are basically the same as with the first version of Frame Generation, just slightly more pronounced. In the built-in <em>Cyberpunk 2077</em> benchmark, the most visible issues are with some bits of barbed-wire fencing, which get smoother-looking and less detailed as you crank up the number of AI-generated frames. But the motion&nbsp;<em>does</em> look fluid and smooth, and the frame rate counts are admittedly impressive.</p>
<p>But as we noted in last year's 4090 review, the xx90 cards portray FG and MFG in the best light possible since the card is already capable of natively rendering such high frame rates. It's on lower-end cards where the shortcomings of the technology become more pronounced. Nvidia might say that the upcoming RTX 5070 is "as fast as a 4090 for $549," and it might be right in terms of the number of frames the card can put up on your screen every second. But responsiveness and visual fidelity on the 4090 will be better every time—AI is a good augmentation for rendered frames, but it's iffy as a <em>replacement</em> for rendered frames.</p>

<h2>A 4090, amped way up</h2>
<figure>
    
          <figcaption>
        <div>
    
    <p>
      Nvidia's GeForce RTX 5090.

              <span>
          Credit:

          
          Andrew Cunningham

                  </span>
          </p>
  </div>
      </figcaption>
      </figure>

<p>The GeForce RTX 5090 is an impressive card—it's the only consumer graphics card to be released in over two years that can outperform the RTX 4090. The main caveats are its sky-high power consumption and sky-high price; by itself, it costs as much (and consumes as much power as) an entire mainstream gaming PC. The card is aimed at people who care about speed way more than they care about price, but it's still worth putting it into context.</p>
<p>The main controversy, as with the 40-series, is how Nvidia talks about its Frame Generation-inflated performance numbers. Frame Generation and Multi-Frame Generation are tools in a toolbox—there will be games where they make things look great and run fast with minimal noticeable impact to visual quality or responsiveness, games where those impacts are more noticeable, and games that never add support for the features at all. (As well-supported as DLSS generally is in new releases, it&nbsp;<em>is</em> incumbent upon game developers to add it—and update it when Nvidia puts out a new version.)</p>

          
                  </div>
                    
        
          
    
    <div>

        
        <div>
          
          
<p>But using those Multi-Frame Generation-inflated FPS numbers to make topline comparisons to last-generation graphics cards just feels disingenuous. No, an RTX 5070 will not be as fast as an RTX 4090 for just $549, because not all games support DLSS MFG, and not all games that&nbsp;<em>do</em> support it will run it well. Frame Generation still needs a good base frame rate to start with, and the slower your card is, the more issues you might notice.</p>
<p>Fuzzy marketing aside, Nvidia is still the undisputed leader in the GPU market, and the RTX 5090 extends that leadership for what will likely be another entire GPU generation, since both <a href="https://arstechnica.com/gadgets/2025/01/new-radeon-rx-9000-gpus-promise-to-fix-two-of-amds-biggest-weaknesses/">AMD</a> and <a href="https://arstechnica.com/gadgets/2024/12/review-intel-arc-b580-is-a-compelling-if-incredibly-tardy-250-midrange-gpu/">Intel</a> are focusing their efforts on higher-volume, lower-cost cards right now. DLSS is still generally better than AMD's FSR, and Nvidia does a good job of getting developers of new AAA game releases to support it. And if you're buying this GPU to do some kind of rendering work or generative AI acceleration, Nvidia's performance and software tools are still superior. The misleading performance claims are frustrating, but Nvidia still gains a lot of real advantages from being as dominant and entrenched as it is.</p>
<h3>The good</h3>
<ul>
<li>Usually 30-something percent faster than an RTX 4090</li>
<li>Redesigned Founders Edition card is less unwieldy than the bricks that were the 4090/4080 design</li>
<li>Adequate cooling, despite the smaller card and higher power use</li>
<li>DLSS Multi-Frame Generation is an intriguing option if you're trying to hit 240 or 360 fps on your high-refresh-rate gaming monitor</li>
</ul>
<h3>The bad</h3>
<ul>
<li>Much higher power consumption than the 4090, which already consumed more power than any other GPU on the market</li>
<li>Frame Generation is good at making a game that's running fast run faster, it's not as good for bringing a slow game up to 60 Hz</li>
<li>Nvidia's misleading marketing around Multi-Frame Generation is frustrating—and will likely be <em>more</em> frustrating for lower-end cards since they aren't getting the same bumps to core count and memory interface that the 5090 gets</li>
</ul>
<h3>The ugly</h3>
<ul>
<li>You can buy a whole lot of PC for $2,000, and we wouldn't bet on this GPU being easy to find at MSRP</li>
</ul>


          
                  </div>

                  
          






  <div>
    

    <p>
      Andrew is a Senior Technology Reporter at Ars Technica, with a focus on consumer tech including computer hardware and in-depth reviews of operating systems like Windows and macOS. Andrew lives in Philadelphia and co-hosts a weekly book podcast called <a href="https://overduepodcast.com/">Overdue</a>.
    </p>
  </div>


  <p>
    <a href="https://arstechnica.com/gadgets/2025/01/review-nvidias-geforce-rtx-5090-is-the-first-gpu-that-can-beat-the-rtx-4090/#comments" title="101 comments">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 80"><defs><clipPath id="bubble-zero_svg__a"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath><clipPath id="bubble-zero_svg__b"><path fill="none" stroke-width="0" d="M0 0h80v80H0z"></path></clipPath></defs><g clip-path="url(#bubble-zero_svg__a)"><g fill="currentColor" clip-path="url(#bubble-zero_svg__b)"><path d="M80 40c0 22.09-17.91 40-40 40S0 62.09 0 40 17.91 0 40 0s40 17.91 40 40"></path><path d="M40 40 .59 76.58C-.67 77.84.22 80 2.01 80H40z"></path></g></g></svg>
    101 Comments
  </a>
      </p>
              </div>
  </article>


  


  


  


  

  </div></div>]]></content:encoded>
        </item>
    </channel>
</rss>